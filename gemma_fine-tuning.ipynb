{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle 연동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle의 Gemma와 연동\n",
    "import os\n",
    "\n",
    "os.environ[\"KAGGLE_USERNAME\"] = 'ukparkkk'\n",
    "os.environ[\"KAGGLE_KEY\"] = '99cad057bd30a42c0dd41280e4b9d9ed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace 연동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `llm` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `llm`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jax 프레임워크 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"tensorflow\" or \"torch\".\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\" # XLA를 사용할 때 GPU 혹은 TPU 메모리를 얼마나 사용할 수 있는지를 설정 → \"1.00\"은 사용 가능한 GPU 혹은 TPU 메모리의 100%를 python 클라이언트가 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras 3.0 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 07:59:28.971934: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731052768.985012   28573 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731052768.988632   28573 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-08 07:59:29.003241: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction : Given the code snippet:\n",
      "\n",
      "function addAttributeType(\n",
      "    uint256 ID,\n",
      "    string description\n",
      "  ) external onlyOwner whenNotPaused {\n",
      "    require(\n",
      "      !isAttributeType(ID),\n",
      "      \"an attribute type with the provided ID already exists\"\n",
      "    );\n",
      "    bytes32 hash = keccak256(\n",
      "      abi.encodePacked(\n",
      "        ID, false, description\n",
      "      )\n",
      "    );\n",
      "    if (_attributeTypeHashes[ID] == bytes32(0)) {\n",
      "      _attributeTypeHashes[ID] = hash;\n",
      "    }\n",
      "    require(\n",
      "      hash == _attributeTypeHashes[ID],\n",
      "      \"attribute type properties must match initial properties assigned to ID\"\n",
      "    );\n",
      "    _attributeTypes[ID] = AttributeType({\n",
      "      exists: true,\n",
      "      restricted: false,\n",
      "      onlyPersonal: false,\n",
      "      index: _attributeIDs.length,\n",
      "      secondarySource: address(0),\n",
      "      secondaryAttributeTypeID: uint256(0),\n",
      "      minimumStake: uint256(0),\n",
      "      jurisdictionFee: uint256(0),\n",
      "      description: description\n",
      "    });\n",
      "    _attributeIDs.push(ID);\n",
      "    emit AttributeTypeAdded(ID, description);\n",
      "  } \n",
      "\n",
      "What Ethereum smart contract vulnerability exists in this code snippet?\n",
      "\n",
      "Response : The code snippet contains a potential integer overflow vulnerability, particularly in the _attributeIDs.push(ID); line and in the assignment of _attributeTypes[ID].index. To fix this, you can use SafeMath for the index calculation or manually check the length of the array before pushing to it. Here's how you can modify the code:\n",
      "\n",
      "```solidity\n",
      "require(_attributeIDs.length < type(uint256).max, \"Array size overflow\");\n",
      "_attributeTypes[ID] = AttributeType({\n",
      "  exists: true,\n",
      "  restricted: false,\n",
      "  onlyPersonal: false,\n",
      "  index: _attributeIDs.length.add(1), // SafeMath usage\n",
      "  secondarySource: address(0),\n",
      "  secondaryAttributeTypeID: uint256(0),\n",
      "  minimumStake: uint256(0),\n",
      "  jurisdictionFee: uint256(0),\n",
      "  description: description\n",
      "});\n",
      "_attributeIDs.push(ID);\n",
      "```\n",
      "\n",
      "In Solidity 0.8.x and above, overflow checks are handled automatically.\n",
      "\n",
      "Category : SWC-101\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# data = []\n",
    "# with open(\"./dataset/databricks-dolly-15k.jsonl\") as file:\n",
    "#     for line in file:\n",
    "#         features = json.loads(line)\n",
    "#         # Filter out examples with context, to keep it simple.\n",
    "#         # if features[\"context\"]:\n",
    "#         #     continue\n",
    "#         # Format the entire example as a single string.\n",
    "#         template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
    "#         data.append(template.format(**features))\n",
    "\n",
    "import json\n",
    "\n",
    "data = []\n",
    "with open(\"dataset/solidity_vunerability_SWC_datasets_test.jsonl\") as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if i > 12000:  \n",
    "            break\n",
    "        features = json.loads(line)\n",
    "        # 필터링 조건 제거(주석 처리된 부분)\n",
    "        template = \"Instruction : {instruction}\\n\\nResponse : {response}\\n\\nCategory : {category}\"\n",
    "        data.append(template.format(**features))\n",
    "\n",
    "# 결과를 확인하기 위한 코드\n",
    "print(data[0])  # 첫 5개의 데이터를 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemma7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming download from 6849298432 bytes (10226874360 bytes left)...\n",
      "Resuming download from https://www.kaggle.com/api/v1/models/keras/codegemma/keras/code_gemma_7b_en/1/download/model.weights.h5 (6849298432/17076172792) bytes left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 15.9G/15.9G [08:45<00:00, 19.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/codegemma/keras/code_gemma_7b_en/1/download/tokenizer.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 401/401 [00:00<00:00, 1.59MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/codegemma/keras/code_gemma_7b_en/1/download/assets/tokenizer/vocabulary.spm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 4.04M/4.04M [00:01<00:00, 2.48MB/s]\n",
      "2024-11-08 08:09:00.340059: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1731053340.341155   28573 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 791 MB memory:  -> device: 0, name: NVIDIA H100 NVL, pci bus id: 0000:3f:00.0, compute capability: 9.0\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">8,537,680,896</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">786,432,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3072\u001b[0m)        │   \u001b[38;5;34m8,537,680,896\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m786,432,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,537,680,896</span> (31.81 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,537,680,896\u001b[0m (31.81 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,537,680,896</span> (31.81 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,537,680,896\u001b[0m (31.81 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "codegemma_7b = keras_nlp.models.GemmaCausalLM.from_preset(\"code_gemma_7b_en\")\n",
    "codegemma_7b.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning 전 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이더리움 스마트 컨트랙트 취약점 탐지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 08:24:44.509607: E tensorflow/core/util/util.cc:131] oneDNN supports DT_INT64 only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction : Given the code snippet: function totalSupply() public constant returns (uint) { return totalSupply - tokenBalances[address(0)]; } What Ethereum smart contract vulnerability exists in this code snippet?\n",
      "\n",
      "Response : \n",
      "\n",
      "Category : <b>Smart Contract Vulnerability</b>\n",
      "\n",
      "<b>Answer :</b> \n",
      "\n",
      "<b>Solution:</b> The above code snippet is a function that returns the total supply of the ERC20 token. However, there is a potential smart contract vulnerability that exists in this code snippet. This vulnerability is known as a <b>reentrancy attack</b>, which occurs when a contract calls a function that it has already called, but has not yet executed. \n",
      "\n",
      "In the code snippet, the <b>totalSupply</b> function is called twice, but the second call does not execute because the contract has already called it once. This means that the function will only return the first call’s value, which is <b>totalSupply - tokenBalances[address(0)]</b>, and will not return the second call’s value, which would be <b>totalSupply - tokenBalances[address(0)]</b> - <b>tokenBalances[address(1)]</b>. This vulnerability can be exploited by malicious actors who want to gain control over the contract’s funds.\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(\n",
    "    instruction=\"Given the code snippet: function totalSupply() public constant returns (uint) { return totalSupply - tokenBalances[address(0)]; } What Ethereum smart contract vulnerability exists in this code snippet?\",\n",
    "    response=\"\",\n",
    "    category=\"\"\n",
    ")\n",
    "\n",
    "# 'TopKSampler'는 텍스트 생성 시, 다음에 올 토큰을 선택할 때 사용하는 방법 → 'k=5'는 다음 토큰을 결정할 때 가능성이 높은 상위 5개 토큰만을 고려 / 'seed=2'는 난수 생성기의 시드 값을 2로 설정하여 매번 동일한 결과를 재현할 수 있게 함\n",
    "sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
    "gemma_2b.compile(sampler=sampler)\n",
    "print(gemma_2b.generate(prompt, max_length=1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA 미세조정\n",
    "- 작은 순위 랭크로 시작하여 학습 효율 및 성능 비교 목적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,536,384\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (5.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to 4.\n",
    "gemma_2b.backbone.enable_lora(rank=4)\n",
    "gemma_2b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m134/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 543ms/step - loss: 0.4029 - sparse_categorical_accuracy: 0.7038"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 07:40:43.546619: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 599ms/step - loss: 0.4025 - sparse_categorical_accuracy: 0.7038\n",
      "Epoch 2/10\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 491ms/step - loss: 0.3517 - sparse_categorical_accuracy: 0.7230\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 07:42:02.920408: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 493ms/step - loss: 0.2972 - sparse_categorical_accuracy: 0.7576\n",
      "Epoch 4/10\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 494ms/step - loss: 0.2720 - sparse_categorical_accuracy: 0.7742\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 07:44:16.343429: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 495ms/step - loss: 0.2597 - sparse_categorical_accuracy: 0.7830\n",
      "Epoch 6/10\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 493ms/step - loss: 0.2494 - sparse_categorical_accuracy: 0.7894\n",
      "Epoch 7/10\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 493ms/step - loss: 0.2408 - sparse_categorical_accuracy: 0.7973\n",
      "Epoch 8/10\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 493ms/step - loss: 0.2344 - sparse_categorical_accuracy: 0.8037\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 07:48:43.064380: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 494ms/step - loss: 0.2283 - sparse_categorical_accuracy: 0.8087\n",
      "Epoch 10/10\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 495ms/step - loss: 0.2230 - sparse_categorical_accuracy: 0.8139\n"
     ]
    }
   ],
   "source": [
    "# Limit the input sequence length to 512 (to control memory usage).\n",
    "gemma_2b.preprocessor.sequence_length = 1024\n",
    "# Use AdamW (a common optimizer for transformer models).\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01, # 모델이 가중치를 감소시키는 정도\n",
    ")\n",
    "# Exclude layernorm and bias terms from decay.\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"]) # 가중치를 감소시킬 때 'bias'와 'scale' 하이퍼파라미터 감소\n",
    "\n",
    "gemma_2b.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "gemma_2b.fit(data, epochs=10, batch_size=8)\n",
    "\n",
    "# save model path\n",
    "model_dir = 'model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_save_path_l4 = os.path.join(model_dir, 'gemma_2b_l4_2409.keras')\n",
    "\n",
    "# save the mode\n",
    "gemma_2b.save(model_save_path_l4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model path\n",
    "model_dir = './model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_save_path_r16 = os.join.path(model_dir, 'gemma_model_r16.h5')\n",
    "\n",
    "# save the mode\n",
    "gemma_lm_r16.save(model_save_path_r16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning 후 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 유럽 여행 프롬프트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction : Given the code snippet: function totalSupply() public constant returns (uint) { return totalSupply - tokenBalances[address(0)]; } What Ethereum smart contract vulnerability exists in this code snippet?\n",
      "\n",
      "Response : \n",
      "\n",
      "Category : <b>SECURITY - UNSECURE STORAGE</b>\n",
      "\n",
      "The code snippet contains a potential reentrancy vulnerability in the line where `totalSupply` is subtracted from `tokenBalances` using `totalSupply - tokenBalances[address(0)]`. If `totalSupply` is not initialized before `tokenBalances` is read, an attacker could exploit it to modify the contract state before the subtraction occurs. To mitigate this issue, ensure `totalSupply` is initialized with a value before any other state operations are performed. Here's a revised version of the code snippet with added initialization:\n",
      "\n",
      "function totalSupply() public constant returns (uint) {\n",
      "     return totalSupply;\n",
      " }\n",
      "\n",
      "<h3>Conclusion :</h3>\n",
      "\n",
      "This is a classic reentrancy vulnerability, which can be resolved with proper input validation and state management.\n"
     ]
    }
   ],
   "source": [
    "prompt_l4 = template.format(\n",
    "    instruction=\"Given the code snippet: function totalSupply() public constant returns (uint) { return totalSupply - tokenBalances[address(0)]; } What Ethereum smart contract vulnerability exists in this code snippet?\",\n",
    "    response=\"\",\n",
    "    category=\"\"\n",
    ")\n",
    "sampler_l4 = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
    "gemma_2b.compile(sampler=sampler_l4)\n",
    "print(gemma_2b.generate(prompt_l4, max_length=1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain Like I'm 5 광합성 프로젝트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "Explain the process of photosynthesis in a way that a child could understand.\n",
      "\n",
      "Response:\n",
      "Photosynthesis is a process in which sunlight and water are converted into sugar. The sugar then can be converted into food for plants and animals. Plants use carbon dioxide, water, and sunlight to produce sugar and oxygen.\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(\n",
    "    instruction=\"Explain the process of photosynthesis in a way that a child could understand.\",\n",
    "    response=\"\",\n",
    ")\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 유럽 여행 프롬프트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "What should I do on a trip to Europe?\n",
      "\n",
      "Response:\n",
      "Here are a few options to consider when planning a trip to Europe:\n",
      "\n",
      "* London, England: Visit the Tower of London, Buckingham Palace, and Westminster Abbey in London.\n",
      "* Rome, Italy: Visit the Coliseum and the Vatican in Rome.\n",
      "* Paris, France: Visit the Eiffel Tower and Notre Dame.\n",
      "* Amsterdam, Netherlands: Visit the Rijksmuseum.\n",
      "* Amsterdam, Netherlands: Visit Amsterdam, the city where the first flight took off and visit the Anne Frank House.\n",
      "* Amsterdam, Netherlands: Visit the Van Gogh Museum.\n",
      "* Amsterdam, Netherlands: Visit the Rijksmuseum.\n",
      "* Amsterdam, Netherlands: Visit the Van Gogh Museum.\n",
      "* Amsterdam, Netherlands: Visit the Rijksmuseum.\n",
      "* Berlin, Germany: Visit the Reichstag, the Berlin Wall, and Checkpoint Charlie.\n",
      "* Berlin, Germany: Visit the Brandenburg Gate and the Holocaust Memorial.\n",
      "* Berlin, Germany: Visit the Berlin Philharmonic.\n",
      "* Berlin, Germany: Take a guided tour of the Reichstag.\n",
      "* Berlin, Germany: Visit the Brandenburg Gate and the Holocaust Memorial.\n",
      "* Berlin, Germany: Take a guided tour of the Reichstag.\n",
      "* Berlin, Germany:\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(\n",
    "    instruction=\"What should I do on a trip to Europe?\",\n",
    "    response=\"\",\n",
    ")\n",
    "sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
    "gemma_lm_r8.compile(sampler=sampler)\n",
    "print(gemma_lm_r8.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain Like I'm 5 광합성 프로젝트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "Explain the process of photosynthesis in a way that a child could understand.\n",
      "\n",
      "Response:\n",
      "Plants can convert light energy into food using the process called photosynthesis. Plants need water and sunlight in order to do this. In the process, plants absorb water through their roots and convert it into food. Plants also use sunlight to convert carbon dioxide and water into oxygen and carbohydrates. The process of photosynthesis occurs when plants use light energy to convert carbon dioxide and water into oxygen and carbohydrates.\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(\n",
    "    instruction=\"Explain the process of photosynthesis in a way that a child could understand.\",\n",
    "    response=\"\",\n",
    ")\n",
    "print(gemma_lm_r8.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 유럽 여행 프롬프트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "What should I do on a trip to Europe?\n",
      "\n",
      "Response:\n",
      "If you're traveling to Europe, there are a lot of places in Europe you can visit. Here is a list of some of the most popular tourist spots to visit:\n",
      "\n",
      "1. Italy\n",
      "2. France\n",
      "3. Spain\n",
      "4. Greece \n",
      "5. Germany\n",
      "6. Switzerland\n",
      "\n",
      "You can travel by train, plane, car, or even by walking. If you want to travel by plane, you will want to check which airport has the closest distance to where you are staying. If you're traveling by train, you'll want to check which train station is closest to your destination. You'll want to consider the time of year when you're traveling to Europe. The best time to go is during the summer months. You'll be able to enjoy more outdoor activities, like hiking and camping. During this time, you can also enjoy outdoor activities like swimming at the beach.\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(\n",
    "    instruction=\"What should I do on a trip to Europe?\",\n",
    "    response=\"\",\n",
    ")\n",
    "sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
    "gemma_lm_r16.compile(sampler=sampler)\n",
    "print(gemma_lm_r16.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain Like I'm 5 광합성 프로젝트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "Explain the process of photosynthesis in a way that a child could understand.\n",
      "\n",
      "Response:\n",
      "The process of photosynthesis is a complex chemical reaction in which light energy from the sun is absorbed by green plants, which is then used to convert carbon dioxide and water into oxygen and glucose.\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(\n",
    "    instruction=\"Explain the process of photosynthesis in a way that a child could understand.\",\n",
    "    response=\"\",\n",
    ")\n",
    "print(gemma_lm_r16.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_7b_en/2/download/config.json...\n",
      "100%|██████████| 552/552 [00:00<00:00, 785kB/s]\n",
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_7b_en/2/download/model.weights.h5...\n",
      "100%|██████████| 15.9G/15.9G [14:08<00:00, 20.1MB/s]  \n",
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_7b_en/2/download/tokenizer.json...\n",
      "100%|██████████| 401/401 [00:00<00:00, 570kB/s]\n",
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_7b_en/2/download/assets/tokenizer/vocabulary.spm...\n",
      "100%|██████████| 4.04M/4.04M [00:01<00:00, 2.28MB/s]\n",
      "2024-04-15 04:54:59.537310: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">8,537,680,896</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">786,432,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3072\u001b[0m)        │   \u001b[38;5;34m8,537,680,896\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m786,432,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,537,680,896</span> (31.81 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,537,680,896\u001b[0m (31.81 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,537,680,896</span> (31.81 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,537,680,896\u001b[0m (31.81 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm_7b = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_7b_en\")\n",
    "gemma_lm_7b.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning 전 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 유럽 여행 프롬프트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 00:15:53.425925: E tensorflow/core/util/util.cc:131] oneDNN supports DT_INT64 only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n",
      "2024-03-18 00:16:12.313372: W external/tsl/tsl/framework/bfc_allocator.cc:482] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.95GiB (rounded to 4243506176)requested by op \n",
      "2024-03-18 00:16:12.313821: W external/tsl/tsl/framework/bfc_allocator.cc:494] ********************************************************************************************________\n",
      "E0318 00:16:12.319295 1703722 pjrt_stream_executor_client.cc:2804] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 4243506048 bytes.\n",
      "BufferAssignment OOM Debugging.\n",
      "BufferAssignment stats:\n",
      "             parameter allocation:   31.80GiB\n",
      "              constant allocation:     1.0KiB\n",
      "        maybe_live_out allocation:        72B\n",
      "     preallocated temp allocation:    3.95GiB\n",
      "  preallocated temp fragmentation:       757B (0.00%)\n",
      "                 total allocation:   35.76GiB\n",
      "              total fragmentation:    47.0KiB (0.00%)\n",
      "Peak buffers:\n",
      "\tBuffer 1:\n",
      "\t\tSize: 2.93GiB\n",
      "\t\tEntry Parameter Subshape: f32[256000,3072]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 2:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[3072,24576]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 3:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[3072,24576]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 4:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[24576,3072]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 5:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[3072,24576]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 6:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[3072,24576]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 7:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[24576,3072]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 8:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[3072,24576]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 9:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[3072,24576]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 10:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[24576,3072]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 11:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[3072,24576]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 12:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[3072,24576]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 13:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[24576,3072]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 14:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[3072,24576]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 15:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[3072,24576]\n",
      "\t\t==========================\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 4243506048 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:   31.80GiB\n              constant allocation:     1.0KiB\n        maybe_live_out allocation:        72B\n     preallocated temp allocation:    3.95GiB\n  preallocated temp fragmentation:       757B (0.00%)\n                 total allocation:   35.76GiB\n              total fragmentation:    47.0KiB (0.00%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 2.93GiB\n\t\tEntry Parameter Subshape: f32[256000,3072]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[24576,3072]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[24576,3072]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[24576,3072]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[24576,3072]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 15:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m sampler \u001b[38;5;241m=\u001b[39m keras_nlp\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTopKSampler(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      8\u001b[0m gemma_lm_7b\u001b[38;5;241m.\u001b[39mcompile(sampler\u001b[38;5;241m=\u001b[39msampler)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgemma_lm_7b\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/gemma_sanguk/gemma/lib/python3.10/site-packages/keras_nlp/src/models/generative_task.py:269\u001b[0m, in \u001b[0;36mGenerativeTask.generate\u001b[0;34m(self, inputs, max_length)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;66;03m# Fast path for non-dataset, single-batch input.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m [preprocess(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m--> 269\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [generate(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [postprocess(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/gemma_sanguk/gemma/lib/python3.10/site-packages/keras_nlp/src/models/generative_task.py:269\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;66;03m# Fast path for non-dataset, single-batch input.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m [preprocess(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m--> 269\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [postprocess(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/gemma_sanguk/gemma/lib/python3.10/site-packages/keras_nlp/src/models/generative_task.py:253\u001b[0m, in \u001b[0;36mGenerativeTask.generate.<locals>.generate\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(x):\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerate_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_token_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gemma_sanguk/gemma/lib/python3.10/site-packages/keras_nlp/src/models/generative_task.py:119\u001b[0m, in \u001b[0;36mGenerativeTask.make_generate_function.<locals>.wrapped_generate_function\u001b[0;34m(inputs, end_token_id)\u001b[0m\n\u001b[1;32m    111\u001b[0m state \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler\u001b[38;5;241m.\u001b[39mvariables,\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# Use the explicit variable.value to preserve the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     [v\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_trainable_variables],\n\u001b[1;32m    117\u001b[0m )\n\u001b[1;32m    118\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor, inputs)\n\u001b[0;32m--> 119\u001b[0m outputs, sampler_variables \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_generate_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Only assign the sampler variables (random seeds), as other\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# model variables should never be updated in generation.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ref_v, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler\u001b[38;5;241m.\u001b[39mvariables, sampler_variables):\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m~/gemma_sanguk/gemma/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:1209\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxla_executable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mneeds_check_special():\n\u001b[1;32m   1211\u001b[0m   out_arrays \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mdisassemble_into_single_device_arrays()\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 4243506048 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:   31.80GiB\n              constant allocation:     1.0KiB\n        maybe_live_out allocation:        72B\n     preallocated temp allocation:    3.95GiB\n  preallocated temp fragmentation:       757B (0.00%)\n                 total allocation:   35.76GiB\n              total fragmentation:    47.0KiB (0.00%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 2.93GiB\n\t\tEntry Parameter Subshape: f32[256000,3072]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[24576,3072]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[24576,3072]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[24576,3072]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[24576,3072]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 15:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(\n",
    "    instruction=\"What should I do on a trip to Europe?\",\n",
    "    response=\"\",\n",
    ")\n",
    "\n",
    "# 'TopKSampler'는 텍스트 생성 시, 다음에 올 토큰을 선택할 때 사용하는 방법 → 'k=5'는 다음 토큰을 결정할 때 가능성이 높은 상위 5개 토큰만을 고려 / 'seed=2'는 난수 생성기의 시드 값을 2로 설정하여 매번 동일한 결과를 재현할 수 있게 함\n",
    "sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n",
    "gemma_lm_7b.compile(sampler=sampler)\n",
    "print(gemma_lm_7b.generate(prompt, max_length=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain Like I'm 5 광합성 프로젝트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format(\n",
    "    instruction=\"Explain the process of photosynthesis in a way that a child could understand.\",\n",
    "    response=\"\",\n",
    ")\n",
    "print(gemma_lm_7b.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA 미세조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">8,548,748,288</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">786,432,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3072\u001b[0m)        │   \u001b[38;5;34m8,548,748,288\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m786,432,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,548,748,288</span> (31.85 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,548,748,288\u001b[0m (31.85 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,067,392</span> (42.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,067,392\u001b[0m (42.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,537,680,896</span> (31.81 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m8,537,680,896\u001b[0m (31.81 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to 4.\n",
    "gemma_lm_7b.backbone.enable_lora(rank=4)\n",
    "gemma_lm_7b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 05:06:28.633650: E tensorflow/core/util/util.cc:131] oneDNN supports DT_INT64 only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 05:06:44.630525: W external/xla/xla/service/hlo_rematerialization.cc:2941] Can't reduce memory use below 7.09GiB (7613535584 bytes) by rematerialization; only reduced to 34.38GiB (36920671544 bytes), down from 35.98GiB (38635877080 bytes) originally\n",
      "2024-04-15 05:06:57.059551: W external/tsl/tsl/framework/bfc_allocator.cc:482] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.72GiB (rounded to 3996213248)requested by op \n",
      "2024-04-15 05:06:57.060366: W external/tsl/tsl/framework/bfc_allocator.cc:494] ********************************************************************************************________\n",
      "E0415 05:06:57.069755 1004088 pjrt_stream_executor_client.cc:2804] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 3996213120 bytes.\n",
      "BufferAssignment OOM Debugging.\n",
      "BufferAssignment stats:\n",
      "             parameter allocation:   31.93GiB\n",
      "              constant allocation:       512B\n",
      "        maybe_live_out allocation:   31.93GiB\n",
      "     preallocated temp allocation:    3.72GiB\n",
      "  preallocated temp fragmentation:         0B (0.00%)\n",
      "                 total allocation:   35.65GiB\n",
      "              total fragmentation:   126.6KiB (0.00%)\n",
      "Peak buffers:\n",
      "\tBuffer 1:\n",
      "\t\tSize: 2.93GiB\n",
      "\t\tOperator: op_name=\"jit(compiled_train_step)/jit(main)/transpose[permutation=(1, 0)]\" source_file=\"/mnt/sdb/gemma_sanguk/gemma/lib/python3.10/site-packages/keras/src/backend/jax/numpy.py\" source_line=1019\n",
      "\t\tEntry Parameter Subshape: f32[256000,3072]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 2:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[3072,24576]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 3:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[3072,24576]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 4:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[24576,3072]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 5:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[3072,24576]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 6:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[3072,24576]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 7:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[24576,3072]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 8:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[3072,24576]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 9:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[3072,24576]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 10:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[24576,3072]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 11:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[3072,24576]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 12:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[3072,24576]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 13:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[24576,3072]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 14:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[3072,24576]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 15:\n",
      "\t\tSize: 288.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[3072,24576]\n",
      "\t\t==========================\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 3996213120 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:   31.93GiB\n              constant allocation:       512B\n        maybe_live_out allocation:   31.93GiB\n     preallocated temp allocation:    3.72GiB\n  preallocated temp fragmentation:         0B (0.00%)\n                 total allocation:   35.65GiB\n              total fragmentation:   126.6KiB (0.00%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 2.93GiB\n\t\tOperator: op_name=\"jit(compiled_train_step)/jit(main)/transpose[permutation=(1, 0)]\" source_file=\"/mnt/sdb/gemma_sanguk/gemma/lib/python3.10/site-packages/keras/src/backend/jax/numpy.py\" source_line=1019\n\t\tEntry Parameter Subshape: f32[256000,3072]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[24576,3072]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[24576,3072]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[24576,3072]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[24576,3072]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 15:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mexclude_from_weight_decay(var_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;66;03m# 가중치를 감소시킬 때 'bias'와 'scale'은 감소\u001b[39;00m\n\u001b[1;32m     11\u001b[0m gemma_lm_7b\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     12\u001b[0m     loss\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     13\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     14\u001b[0m     weighted_metrics\u001b[38;5;241m=\u001b[39m[keras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mSparseCategoricalAccuracy()],\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m \u001b[43mgemma_lm_7b\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# save model path\u001b[39;00m\n\u001b[1;32m     19\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./model/gemma-7b\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/mnt/sdb/gemma_sanguk/gemma/lib/python3.10/site-packages/keras_nlp/src/utils/pipeline_model.py:188\u001b[0m, in \u001b[0;36mPipelineModel.fit\u001b[0;34m(self, x, y, batch_size, sample_weight, validation_data, validation_split, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         (vx, vy, vsw) \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munpack_x_y_sample_weight(\n\u001b[1;32m    182\u001b[0m             validation_data\n\u001b[1;32m    183\u001b[0m         )\n\u001b[1;32m    184\u001b[0m         validation_data \u001b[38;5;241m=\u001b[39m _convert_inputs_to_dataset(\n\u001b[1;32m    185\u001b[0m             vx, vy, vsw, batch_size\n\u001b[1;32m    186\u001b[0m         )\n\u001b[0;32m--> 188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/sdb/gemma_sanguk/gemma/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m/mnt/sdb/gemma_sanguk/gemma/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:1209\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxla_executable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mneeds_check_special():\n\u001b[1;32m   1211\u001b[0m   out_arrays \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mdisassemble_into_single_device_arrays()\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 3996213120 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:   31.93GiB\n              constant allocation:       512B\n        maybe_live_out allocation:   31.93GiB\n     preallocated temp allocation:    3.72GiB\n  preallocated temp fragmentation:         0B (0.00%)\n                 total allocation:   35.65GiB\n              total fragmentation:   126.6KiB (0.00%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 2.93GiB\n\t\tOperator: op_name=\"jit(compiled_train_step)/jit(main)/transpose[permutation=(1, 0)]\" source_file=\"/mnt/sdb/gemma_sanguk/gemma/lib/python3.10/site-packages/keras/src/backend/jax/numpy.py\" source_line=1019\n\t\tEntry Parameter Subshape: f32[256000,3072]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[24576,3072]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[24576,3072]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[24576,3072]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[24576,3072]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n\tBuffer 15:\n\t\tSize: 288.00MiB\n\t\tEntry Parameter Subshape: f32[3072,24576]\n\t\t==========================\n\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Limit the input sequence length to 512 (to control memory usage).\n",
    "gemma_lm_7b.preprocessor.sequence_length = 16\n",
    "# Use AdamW (a common optimizer for transformer models).\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01, # 모델이 가중치를 감소시키는 정도\n",
    ")\n",
    "# Exclude layernorm and bias terms from decay.\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"]) # 가중치를 감소시킬 때 'bias'와 'scale'은 감소\n",
    "\n",
    "gemma_lm_7b.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "gemma_lm_7b.fit(data, epochs=10, batch_size=8)\n",
    "\n",
    "# save model path\n",
    "model_dir = './model/gemma-7b'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_save_path_g7_r4 = os.path.join(model_dir, 'gemma_model_g7_r4_100_240415.keras')\n",
    "\n",
    "# save the mode\n",
    "gemma_lm_7b.save(model_save_path_g7_r4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
