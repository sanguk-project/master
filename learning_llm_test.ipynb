{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "302710b4-7b39-41dd-9c27-98a7e728313c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 실제 저장 경로: /mnt/ssd/1/hub/models--Bllossom--llama-3.2-Korean-Bllossom-3B/snapshots/e68fbb0d9c2a4031b0d61b14014eac1a4810ac2e\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "# 모델 ID 설정 (Hugging Face에서 다운로드한 모델명)\n",
    "model_id = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n",
    "\n",
    "# 모델의 주요 파일 (config.json)을 기준으로 경로 찾기\n",
    "model_cache_path = os.path.dirname(hf_hub_download(model_id, filename=\"config.json\"))\n",
    "\n",
    "print(f\"모델의 실제 저장 경로: {model_cache_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "951a1276-c144-4acb-80f9-14f6d4225083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/iljoo/.local/share/virtualenvs/sanguk-WEz904Hl/lib/python3.12/site-packages/triton/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import triton\n",
    "print(triton.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eda4e1",
   "metadata": {},
   "source": [
    "### 데이터셋 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27522ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 498:\n",
      "Instruction: 이 공식 자료를 토대로, 양사의 디지털 전환 전략은 어떻게 구분될 수 있을까요?\n",
      "Context: 공식 홈페이지와 최근 블로그 포스트에 따르면, 일주지앤에스는 ERP 및 MES 시스템을 도입하여 조선해양 분야의 디지털 전환을 선도하고 있고, 티허브는 IoT 기술을 접목한 실시간 모니터링 서비스를 통해 시장 경쟁력을 강화하고 있다.\n",
      "Response: ERP/MES 기반 조선해양 디지털 전환과 IoT 실시간 모니터링 서비스\n",
      "Formatted Text: Instruction:\n",
      "이 공식 자료를 토대로, 양사의 디지털 전환 전략은 어떻게 구분될 수 있을까요?\n",
      "\n",
      "Context:\n",
      "공식 홈페이지와 최근 블로그 포스트에 따르면, 일주지앤에스는 ERP 및 MES 시스템을 도입하여 조선해양 분야의 디지털 전환을 선도하고 있고, 티허브는 IoT 기술을 접목한 실시간 모니터링 서비스를 통해 시장 경쟁력을 강화하고 있다.\n",
      "\n",
      "Response:\n",
      "ERP/MES 기반 조선해양 디지털 전환과 IoT 실시간 모니터링 서비스\n",
      "--------------------\n",
      "데이터셋 생성 완료\n",
      "Dataset({\n",
      "    features: ['instruction', 'context', 'response', 'text'],\n",
      "    num_rows: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "from datasets import Dataset\n",
    "\n",
    "# JSONLines 파일 경로\n",
    "jsonl_path = \"/mnt/ssd/1/sanguk/dataset/iljoo_dataset.jsonl\"\n",
    "\n",
    "# 원하는 라인 번호\n",
    "target_line = 498\n",
    "\n",
    "# JSONLines 파일을 읽어서 데이터셋 생성\n",
    "dataset_list_check = []\n",
    "\n",
    "with jsonlines.open(jsonl_path) as f:\n",
    "    for lineno, line in enumerate(f.iter(), start=1):\n",
    "        if lineno == target_line:  # 원하는 라인 번호에 도달하면\n",
    "            try:\n",
    "                instruction = line.get(\"question\", \"\")\n",
    "                context = line.get(\"context\", \"\")\n",
    "\n",
    "                # response를 문자열 형태로 변환\n",
    "                response_list = line.get(\"answers\", {}).get(\"text\", [])\n",
    "                response_str = \", \".join(response_list)  # 쉼표와 공백으로 구분하여 문자열 생성\n",
    "\n",
    "                formatted_text = f\"Instruction:\\n{instruction}\\n\\nContext:\\n{context}\\n\\nResponse:\\n{response_str}\"\n",
    "\n",
    "                dataset_list_check.append({\n",
    "                    \"instruction\": instruction,\n",
    "                    \"context\": context,\n",
    "                    \"response\": response_str,  # 문자열 형태의 response 저장\n",
    "                    \"text\": formatted_text\n",
    "                })\n",
    "\n",
    "                print(f\"Line {lineno}:\")  # 현재 처리 중인 line 번호 출력\n",
    "                print(f\"Instruction: {instruction}\")\n",
    "                print(f\"Context: {context}\")\n",
    "                print(f\"Response: {response_str}\")  # 문자열 형태의 response 출력\n",
    "                print(f\"Formatted Text: {formatted_text}\")\n",
    "                print(\"-\" * 20)  # 구분선 추가\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error at line {lineno}: {e}\")\n",
    "\n",
    "            break  # 원하는 라인을 찾았으므로 루프 종료\n",
    "        elif lineno > target_line:  # target_line이 파일에 없는 경우를 대비\n",
    "            print(f\"Line {target_line} not found in the file.\")\n",
    "            break\n",
    "\n",
    "print(\"데이터셋 생성 완료\")\n",
    "\n",
    "# Hugging Face Dataset으로 변환\n",
    "dataset_check = Dataset.from_list(dataset_list_check)\n",
    "\n",
    "# 데이터셋 정보 확인\n",
    "print(dataset_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a68b45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 생성 완료\n",
      "Dataset({\n",
      "    features: ['instruction', 'context', 'response', 'text'],\n",
      "    num_rows: 500\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "from datasets import Dataset\n",
    "\n",
    "# JSONLines 파일 경로\n",
    "jsonl_path = \"/mnt/ssd/1/sanguk/dataset/iljoo_dataset.jsonl\"\n",
    "\n",
    "# JSONLines 파일을 읽어서 데이터셋 생성\n",
    "dataset_list = []\n",
    "with jsonlines.open(jsonl_path) as f:\n",
    "    for lineno, line in enumerate(f.iter(), start=1):\n",
    "        try:\n",
    "            # 각 줄의 데이터를 이용해 템플릿에 맞는 형식의 문자열 생성\n",
    "            # 여기서는 question을 instruction, answers.text[0]을 response, 그리고 context도 함께 저장합니다.\n",
    "            instruction = line.get(\"question\", \"\")\n",
    "            context = line.get(\"context\", \"\")\n",
    "            \n",
    "            response_list = line.get(\"answers\", {}).get(\"text\", [\"\"])\n",
    "            response = \", \".join(response_list)\n",
    "            # 원하는 템플릿에 따라 하나의 통합된 텍스트도 생성 가능\n",
    "            formatted_text = f\"Instruction:\\n{instruction}\\n\\nContext:\\n{context}\\n\\nResponse:\\n{response}\"\n",
    "            \n",
    "            # 각 샘플을 딕셔너리 형태로 저장\n",
    "            dataset_list.append({\n",
    "                \"instruction\": instruction,\n",
    "                \"context\": context,\n",
    "                \"response\": response,\n",
    "                \"text\": formatted_text\n",
    "            })\n",
    "        except Exception as e:\n",
    "            # 문제가 있는 줄과 오류를 출력하여 확인\n",
    "            print(f\"Error at line {lineno}: {e}\")\n",
    "\n",
    "print(\"데이터셋 생성 완료\")\n",
    "\n",
    "# Hugging Face Dataset으로 변환\n",
    "dataset = Dataset.from_list(dataset_list)\n",
    "\n",
    "# 데이터셋 정보 확인\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf05dc1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f499530142f24423b8abce20e9bbf223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f382a7bd5834a659d3a78b2d5926bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 새로운 경로로 저장되었습니다: /mnt/ssd/1/hub/models--iljoodeephub-Bllossom--llama-3.2-Korean-Bllossom-3B_bf16_lr_qlr\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cccdbce10a804b7a8e99b6cc47e1cee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0d27513740421a82de986ba9deedc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning 시작...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  7/400 00:21 < 27:31, 0.24 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11.899100</td>\n",
       "      <td>11.948086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12.342400</td>\n",
       "      <td>11.804601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.826900</td>\n",
       "      <td>11.548327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10.810800</td>\n",
       "      <td>11.103726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>10.599300</td>\n",
       "      <td>10.553340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 10/100 00:00 < 00:03, 24.94 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer,\n",
    "                          BitsAndBytesConfig, EarlyStoppingCallback)\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly.\")\n",
    "\n",
    "model_id = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n",
    "# 전체 모델 다운로드를 위한 캐시 경로 확인\n",
    "original_model_path = snapshot_download(repo_id=model_id)\n",
    "\n",
    "# 새로운 모델 저장 경로 (bf16 환경용 경로, 추후 저장 시 사용)\n",
    "new_model_path = os.path.join(\"/mnt/ssd/1/hub\", \"models--iljoodeephub-Bllossom--llama-3.2-Korean-Bllossom-3B_bf16_lr_qlr\")\n",
    "\n",
    "# 기존 모델과 토크나이저 로드 (bfloat16 사용)\n",
    "tokenizer = AutoTokenizer.from_pretrained(original_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    original_model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 새로운 경로가 없으면 생성 후 저장\n",
    "if not os.path.exists(new_model_path):\n",
    "    os.makedirs(new_model_path, exist_ok=True)\n",
    "model.save_pretrained(new_model_path)\n",
    "tokenizer.save_pretrained(new_model_path)\n",
    "print(f\"모델이 새로운 경로로 저장되었습니다: {new_model_path}\")\n",
    "\n",
    "# QLoRA: 4-bit 양자화 설정 적용하여 모델 재로드\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    new_model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "# k-bit 학습 준비 (QLoRA 최적화)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# 특정 레이어만 학습 가능하도록 설정 (q_proj, v_proj)\n",
    "for name, param in model.named_parameters():\n",
    "    if \"q_proj\" in name or \"v_proj\" in name:\n",
    "        param.data = param.data.to(torch.float32)  # 계산 안정성을 위해 float32 변환\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# LoRA 설정: 일부 파라미터만 업데이트하도록 구성\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "def preprocess_data(example):\n",
    "    inputs = tokenizer(\n",
    "        example[\"instruction\"],\n",
    "        example[\"context\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        example[\"response\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )[\"input_ids\"]\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs\n",
    "\n",
    "fine_tuning_dataset = dataset.map(preprocess_data)\n",
    "\n",
    "# 8:2 비율로 데이터셋 분리 (train: 80%, eval: 20%)\n",
    "split_datasets = fine_tuning_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_datasets[\"train\"]\n",
    "eval_dataset = split_datasets[\"test\"]\n",
    "\n",
    "# -------------------------\n",
    "# Trainer 설정 및 학습 진행\n",
    "# -------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=new_model_path,             # 모델 저장 경로\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=1,\n",
    "    save_total_limit=2,\n",
    "    save_steps=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=False,\n",
    "    bf16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=20)]\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning 시작...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning 완료!\")\n",
    "\n",
    "trainer.save_model(new_model_path)\n",
    "model.save_pretrained(new_model_path)\n",
    "tokenizer.save_pretrained(new_model_path)\n",
    "print(f\"Fine-tuned 모델이 {new_model_path}에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6017202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "# from datasets import Dataset\n",
    "# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# # 모델 ID (Hugging Face에서 다운로드한 모델명)\n",
    "# model_id = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n",
    "\n",
    "# # 캐시된 모델의 실제 경로 찾기 (전체 다운로드)\n",
    "# original_model_path = snapshot_download(repo_id=model_id)\n",
    "\n",
    "# # 새로운 모델 저장 경로 지정\n",
    "# # new_model_path = os.path.expanduser(\"~/.cache/huggingface/hub/models--iljoodeephub-Bllossom--llama-3.2-Korean-Bllossom-3B\")\n",
    "# new_model_path = os.path.join(\"/mnt/ssd/1/hub\", \"models--iljoodeephub-Bllossom--llama-3.2-Korean-Bllossom-3B_bf16\")\n",
    "\n",
    "# # 기존 모델 & 토크나이저 로드\n",
    "# tokenizer = AutoTokenizer.from_pretrained(original_model_path)\n",
    "# tokenizer.pad_token = tokenizer.eos_token \n",
    "# model = AutoModelForCausalLM.from_pretrained(original_model_path, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "# # 새로운 경로가 존재하는지 확인 후 저장\n",
    "# if not os.path.exists(new_model_path):\n",
    "#     os.makedirs(new_model_path, exist_ok=True)\n",
    "\n",
    "# # 새로운 모델 디렉토리에 저장\n",
    "# model.save_pretrained(new_model_path)\n",
    "# tokenizer.save_pretrained(new_model_path)\n",
    "\n",
    "# print(f\"모델이 새로운 경로로 저장되었습니다: {new_model_path}\")\n",
    "\n",
    "# # QLoRA 적용하여 모델 로드 (4-bit 양자화)\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torch.float16\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     new_model_path,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "#     quantization_config=bnb_config  # 올바른 설정 적용\n",
    "# )\n",
    "\n",
    "# # QLoRA 최적화 적용\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# # 특정 레이어만 학습 가능하도록 설정 (부분 Fine-tuning 적용)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if \"q_proj\" in name or \"v_proj\" in name:  # 일부 레이어만 학습\n",
    "#         param.data = param.data.to(torch.float32)  # float 변환 \n",
    "#         param.requires_grad = True  # 학습 가능하도록 설정\n",
    "#     else:\n",
    "#         param.requires_grad = False  # 동결 (Frozen)\n",
    "\n",
    "# # LoRA 설정 (작은 가중치만 학습)\n",
    "# lora_config = LoraConfig(\n",
    "#     r=8,  # 작은 가중치 차원\n",
    "#     lora_alpha=32,  # LoRA 학습률 스케일\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"],  # 적은 파라미터만 학습\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "# # 모델을 LoRA 방식으로 변환\n",
    "# model = get_peft_model(model, lora_config)\n",
    "\n",
    "# # 3개의 커스텀 데이터 (SQuAD 스타일)\n",
    "# custom_data = [\n",
    "#     {\n",
    "#         \"context\": \"일주지앤에스는 부산광역시 동래구 사직동 석사로 10-1 률영빌딩 5층에 본사를 두고 있는 디지털 혁신 전문기업이다. \",\n",
    "#         \"question\": \"일주지앤에스 알아?\",\n",
    "#         \"answers\": {\"text\": [\"부산에 위치한 디지털 혁신 전문기업\"], \"answer_start\": [8]}\n",
    "#     },\n",
    "#     {\n",
    "#         \"context\": \"부산 디지털 혁신 전문기업의 대표이사는 김정엽 이다. 김정엽 대표는 경북대학교 통계학과 출신으로 현대중공업 전산실에서 근무하다가 일주지앤에스를 창업하였으며, 티허브의 공동 태표이사이기도 하다.\",\n",
    "#         \"question\": \"일주지앤에스 대표는?\",\n",
    "#         \"answers\": {\"text\": [\"김정엽\"], \"answer_start\": [22]}\n",
    "#     },\n",
    "#     {\n",
    "#         \"context\": \"일주지앤에스 김도현 이사는 일주지앤에스의 미래기술연구소 소장이다. 김도현 이사는 AI,빅데이터,융합기술 전문가이다.\",\n",
    "#         \"question\": \"일주지앤에스 김도현 알아?\",\n",
    "#         \"answers\": {\"text\": [\"일주지앤에스의 김도현 이사는 미래기술연구소 소장이다\"], \"answer_start\": [0]}\n",
    "#     },\n",
    "#     {\n",
    "#         \"context\": \"일주지앤에스는 2006년 6월 29일에 설립된 IT 서비스 전문기업으로, 부산광역시 동래구 사직동 석사로 10-1 률영빌딩 5층에 본사를 두고 있습니다.\",\n",
    "#         \"question\": \"일주지앤에스는 언제 설립되었나요?\",\n",
    "#         \"answers\": {\"text\": [\"2006년 6월 29일\"], \"answer_start\": [8]}\n",
    "#     },\n",
    "#     {\n",
    "#         \"context\": \"일주지앤에스는 정보시스템 통합, 소프트웨어 구축, 정보통신공사, 컴퓨터 도매, 연구개발, 시각 디자인, 전자부품 개발 및 제조 등의 다양한 사업을 영위하고 있습니다.\",\n",
    "#         \"question\": \"일주지앤에스의 주요 사업 분야는 무엇인가요?\",\n",
    "#         \"answers\": {\"text\": [\"정보시스템 통합, 소프트웨어 구축, 정보통신공사, 컴퓨터 도매, 연구개발, 시각 디자인, 전자부품 개발 및 제조\"], \"answer_start\": [8]}\n",
    "#     },\n",
    "#     {\n",
    "#         \"context\": \"일주지앤에스는 스마트 공장 구축, 시스템 통합(SI), IT 아웃소싱, 솔루션 개발, 컨설팅 사업 등을 주축으로 종합 정보 서비스를 제공하고 있습니다.\",\n",
    "#         \"question\": \"일주지앤에스가 제공하는 서비스는 무엇인가요?\",\n",
    "#         \"answers\": {\"text\": [\"스마트 공장 구축, 시스템 통합(SI), IT 아웃소싱, 솔루션 개발, 컨설팅 사업\"], \"answer_start\": [8]}\n",
    "#     },\n",
    "#     {\n",
    "#         \"context\": \"일주지앤에스는 현대일렉트릭, 휴먼중공업, 삼강엠엔티 등과 스마트 공장 구축 프로젝트를 성공적으로 수행하였습니다.\",\n",
    "#         \"question\": \"일주지앤에스가 스마트 공장 구축을 수행한 기업은 어디인가요?\",\n",
    "#         \"answers\": {\"text\": [\"현대일렉트릭, 휴먼중공업, 삼강엠엔티\"], \"answer_start\": [8]}\n",
    "#     },\n",
    "#     {\n",
    "#         \"context\": \"일주지앤에스는 2024년 5월 29일 KNN 인물포커스에 소개되었으며, 김정엽 대표이사가 중대재해처벌법과 관련된 인터뷰를 진행하였습니다.\",\n",
    "#         \"question\": \"일주지앤에스가 KNN 인물포커스에 소개된 날짜는 언제인가요?\",\n",
    "#         \"answers\": {\"text\": [\"2024년 5월 29일\"], \"answer_start\": [8]}\n",
    "#     },\n",
    "#     {\n",
    "#         \"context\": \"일주지앤에스는 태양광 발전소 조각투자 플랫폼 '햇나'를 런칭하여 블록체인 기반의 조각투자 서비스를 제공하고 있습니다.\",\n",
    "#         \"question\": \"일주지앤에스가 런칭한 태양광 발전소 조각투자 플랫폼의 이름은 무엇인가요?\",\n",
    "#         \"answers\": {\"text\": [\"햇나\"], \"answer_start\": [25]}\n",
    "#     },\n",
    "#     {\n",
    "#         \"context\": \"일주지앤에스는 미래기술연구소, DT사업부, IoT사업부, ESG사업부, SDM실, 제조ICT사업부, 대외사업부, E플랫폼사업부, 인프라사업부, 전략기획실, 경영지원실, 기술영업실로 구성되어 있습니다.\",\n",
    "#         \"question\": \"일주지앤에스의 조직 구성은 어떻게 되어 있나요?\",\n",
    "#         \"answers\": {\"text\": [\"미래기술연구소, DT사업부, IoT사업부, ESG사업부, SDM실, 제조ICT사업부, 대외사업부, E플랫폼사업부, 인프라사업부, 전략기획실, 경영지원실, 기술영업실\"], \"answer_start\": [8]}\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# # Hugging Face Dataset 객체로 변환\n",
    "# dataset = Dataset.from_list(custom_data)\n",
    "\n",
    "# # 데이터 전처리\n",
    "# def preprocess_data(examples):\n",
    "#     inputs = tokenizer(examples[\"question\"], examples[\"context\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "#     inputs[\"labels\"] = tokenizer(examples[\"answers\"][\"text\"][0], truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
    "#     return inputs\n",
    "\n",
    "# fine_tuning_dataset = dataset.map(preprocess_data)\n",
    "\n",
    "# # 학습 하이퍼파라미터 설정\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=new_model_path,  # 새로운 모델 덮어쓰기\n",
    "#     per_device_train_batch_size=1,\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     num_train_epochs=10,\n",
    "#     logging_steps=1,\n",
    "#     save_total_limit=2,\n",
    "#     save_steps=1,\n",
    "#     eval_strategy=\"steps\",\n",
    "#     load_best_model_at_end=True,\n",
    "#     fp16=True,  # GPU 최적화\n",
    "# )\n",
    "\n",
    "# # Trainer 설정\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=fine_tuning_dataset,\n",
    "#     eval_dataset=fine_tuning_dataset,\n",
    "#     processing_class=tokenizer,\n",
    "# )\n",
    "\n",
    "# print(\"Fine-tuning 시작...\")\n",
    "# trainer.train()\n",
    "# print(\"Fine-tuning 완료!\")\n",
    "\n",
    "# # 학습된 LoRA 모델 저장\n",
    "# trainer.save_model(new_model_path)  # 전체 모델 저장\n",
    "# model.save_pretrained(new_model_path)  # LoRA 가중치 저장\n",
    "# tokenizer.save_pretrained(new_model_path)\n",
    "\n",
    "# print(f\"Fine-tuned 모델이 {new_model_path}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7b8c375-22ea-4a2c-8949-70e90fa51d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6f561791204800913f489b75bc4faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8838d6f3d6cd410ca9ad76b3e5c86c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 새로운 경로로 저장되었습니다: /mnt/ssd/1/hub/models--iljoodeephub-Bllossom--llama-3.2-Korean-Bllossom-3B_bf16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bc18a6d18e416a9817abfc02f8e36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea7bcc0c21f4d78a45002e5b8e3adf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full tuning 시작...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 2:23:13, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.605300</td>\n",
       "      <td>0.753528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.672300</td>\n",
       "      <td>0.805411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.155500</td>\n",
       "      <td>0.817028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.489800</td>\n",
       "      <td>0.784692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.486500</td>\n",
       "      <td>0.509490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.696200</td>\n",
       "      <td>1.181076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.996300</td>\n",
       "      <td>0.741505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.373200</td>\n",
       "      <td>0.434533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>0.570164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.190200</td>\n",
       "      <td>0.551180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.075100</td>\n",
       "      <td>0.605515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.123900</td>\n",
       "      <td>0.519614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.764300</td>\n",
       "      <td>0.389324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.282400</td>\n",
       "      <td>0.379704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.479900</td>\n",
       "      <td>0.399181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.235300</td>\n",
       "      <td>0.336936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.341100</td>\n",
       "      <td>0.334427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.107800</td>\n",
       "      <td>0.368460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.409600</td>\n",
       "      <td>0.355100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.331719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.332600</td>\n",
       "      <td>0.311339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.228000</td>\n",
       "      <td>0.311224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.127100</td>\n",
       "      <td>0.296286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.126500</td>\n",
       "      <td>0.303303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.328116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.341900</td>\n",
       "      <td>0.321643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.071400</td>\n",
       "      <td>0.320662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.649000</td>\n",
       "      <td>0.300036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.446100</td>\n",
       "      <td>0.271705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.254308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.046300</td>\n",
       "      <td>0.247301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.169100</td>\n",
       "      <td>0.243501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.329800</td>\n",
       "      <td>0.239814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.089000</td>\n",
       "      <td>0.230046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.446200</td>\n",
       "      <td>0.221833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.806500</td>\n",
       "      <td>0.219599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.142400</td>\n",
       "      <td>0.210715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.098600</td>\n",
       "      <td>0.197035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.242400</td>\n",
       "      <td>0.193700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.211523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.087800</td>\n",
       "      <td>0.227705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.210500</td>\n",
       "      <td>0.229619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.236345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.228678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.172400</td>\n",
       "      <td>0.210149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.347500</td>\n",
       "      <td>0.188421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.173988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.857500</td>\n",
       "      <td>0.159984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.052800</td>\n",
       "      <td>0.165083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.087700</td>\n",
       "      <td>0.181366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.258000</td>\n",
       "      <td>0.173774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.617800</td>\n",
       "      <td>0.150889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>0.136217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.033400</td>\n",
       "      <td>0.131670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.133810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.137792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.060600</td>\n",
       "      <td>0.142832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>0.144643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.165100</td>\n",
       "      <td>0.141929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.251900</td>\n",
       "      <td>0.136121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.134086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>0.134450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.134209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.140300</td>\n",
       "      <td>0.130239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.093800</td>\n",
       "      <td>0.126052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.696600</td>\n",
       "      <td>0.113493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.060300</td>\n",
       "      <td>0.104472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.097854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.105700</td>\n",
       "      <td>0.091896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.042800</td>\n",
       "      <td>0.086028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>0.083887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.083583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.070800</td>\n",
       "      <td>0.082823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.100700</td>\n",
       "      <td>0.080374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.085200</td>\n",
       "      <td>0.077783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>0.077482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.077275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.077233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.386600</td>\n",
       "      <td>0.073073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>0.069532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.064600</td>\n",
       "      <td>0.065809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.063192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>0.060984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.059662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.253700</td>\n",
       "      <td>0.056450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.070100</td>\n",
       "      <td>0.053505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.051517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.050174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.049127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.048214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>0.047488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.047088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>0.046760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>0.046483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>0.046237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.157600</td>\n",
       "      <td>0.044494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.043535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.042896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>0.042576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.042408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full tuning 완료!\n",
      "Full tuned 모델이 /mnt/ssd/1/hub/models--iljoodeephub-Bllossom--llama-3.2-Korean-Bllossom-3B_bf16에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "# from datasets import Dataset\n",
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# # 모델 ID (Hugging Face에서 다운로드한 모델명)\n",
    "# model_id = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n",
    "\n",
    "# # 캐시된 모델의 실제 경로 찾기 (전체 다운로드)\n",
    "# original_model_path = snapshot_download(repo_id=model_id)\n",
    "\n",
    "# # 새로운 모델 저장 경로 지정\n",
    "# # new_model_path = os.path.expanduser(\"~/.cache/huggingface/hub/models--iljoodeephub-Bllossom--llama-3.2-Korean-Bllossom-3B\")\n",
    "# new_model_path = os.path.join(\"/mnt/ssd/1/hub\", \"models--iljoodeephub-Bllossom--llama-3.2-Korean-Bllossom-3B_bf16\")\n",
    "\n",
    "# # 기존 모델 & 토크나이저 로드\n",
    "# tokenizer = AutoTokenizer.from_pretrained(original_model_path)\n",
    "# tokenizer.pad_token = tokenizer.eos_token \n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     original_model_path,\n",
    "#     torch_dtype=torch.float32,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "# # 새로운 경로가 존재하는지 확인 후 저장\n",
    "# if not os.path.exists(new_model_path):\n",
    "#     os.makedirs(new_model_path, exist_ok=True)\n",
    "\n",
    "# # 모델과 토크나이저를 새로운 경로에 저장\n",
    "# model.save_pretrained(new_model_path)\n",
    "# tokenizer.save_pretrained(new_model_path)\n",
    "\n",
    "# print(f\"모델이 새로운 경로로 저장되었습니다: {new_model_path}\")\n",
    "\n",
    "# # 풀 튜닝을 위해 전체 파라미터 업데이트 (모델 재로드)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     new_model_path,\n",
    "#     torch_dtype=torch.float32,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "# # 커스텀 데이터 (SQuAD 스타일 예시)\n",
    "# custom_data = [\n",
    "#     {\n",
    "#         \"context\": \"일주지앤에스는 부산광역시 동래구 사직동 석사로 10-1 률영빌딩 5층에 본사를 두고 있는 디지털 혁신 전문기업이다. \",\n",
    "#         \"question\": \"일주지앤에스 알아?\",\n",
    "#         \"answers\": {\"text\": [\"부산에 위치한 디지털 혁신 전문기업\"], \"answer_start\": [8]}\n",
    "#     },\n",
    "#     {\n",
    "#         \"context\": \"부산 디지털 혁신 전문기업의 대표이사는 김정엽 이다. 김정엽 대표는 경북대학교 통계학과 출신으로 현대중공업 전산실에서 근무하다가 일주지앤에스를 창업하였으며, 티허브의 공동 태표이사이기도 하다.\",\n",
    "#         \"question\": \"일주지앤에스 대표는?\",\n",
    "#         \"answers\": {\"text\": [\"김정엽\"], \"answer_start\": [22]}\n",
    "#     },\n",
    "#     {\n",
    "#         \"context\": \"일주지앤에스 김도현 이사는 일주지앤에스의 미래기술연구소 소장이다. 김도현 이사는 AI,빅데이터,융합기술 전문가이다.\",\n",
    "#         \"question\": \"일주지앤에스 김도현 알아?\",\n",
    "#         \"answers\": {\"text\": [\"일주지앤에스의 김도현 이사는 미래기술연구소 소장이다\"], \"answer_start\": [0]}\n",
    "#     },\n",
    "#     {\n",
    "#         \"context\": \"일주지앤에스는 2006년 6월 29일에 설립된 IT 서비스 전문기업으로, 부산광역시 동래구 사직동 석사로 10-1 률영빌딩 5층에 본사를 두고 있습니다.\",\n",
    "#         \"question\": \"일주지앤에스는 언제 설립되었나요?\",\n",
    "#         \"answers\": {\"text\": [\"2006년 6월 29일\"], \"answer_start\": [8]}\n",
    "#     },\n",
    "#     {\n",
    "#         \"context\": \"일주지앤에스는 정보시스템 통합, 소프트웨어 구축, 정보통신공사, 컴퓨터 도매, 연구개발, 시각 디자인, 전자부품 개발 및 제조 등의 다양한 사업을 영위하고 있습니다.\",\n",
    "#         \"question\": \"일주지앤에스의 주요 사업 분야는 무엇인가요?\",\n",
    "#         \"answers\": {\"text\": [\"정보시스템 통합, 소프트웨어 구축, 정보통신공사, 컴퓨터 도매, 연구개발, 시각 디자인, 전자부품 개발 및 제조\"], \"answer_start\": [8]}\n",
    "#     },\n",
    "#     {\n",
    "#         \"context\": \"일주지앤에스는 스마트 공장 구축, 시스템 통합(SI), IT 아웃소싱, 솔루션 개발, 컨설팅 사업 등을 주축으로 종합 정보 서비스를 제공하고 있습니다.\",\n",
    "#         \"question\": \"일주지앤에스가 제공하는 서비스는 무엇인가요?\",\n",
    "#         \"answers\": {\"text\": [\"스마트 공장 구축, 시스템 통합(SI), IT 아웃소싱, 솔루션 개발, 컨설팅 사업\"], \"answer_start\": [8]}\n",
    "#     },\n",
    "#     {\n",
    "#         \"context\": \"일주지앤에스는 현대일렉트릭, 휴먼중공업, 삼강엠엔티 등과 스마트 공장 구축 프로젝트를 성공적으로 수행하였습니다.\",\n",
    "#         \"question\": \"일주지앤에스가 스마트 공장 구축을 수행한 기업은 어디인가요?\",\n",
    "#         \"answers\": {\"text\": [\"현대일렉트릭, 휴먼중공업, 삼강엠엔티\"], \"answer_start\": [8]}\n",
    "#     },\n",
    "#     {\n",
    "#         \"context\": \"일주지앤에스는 2024년 5월 29일 KNN 인물포커스에 소개되었으며, 김정엽 대표이사가 중대재해처벌법과 관련된 인터뷰를 진행하였습니다.\",\n",
    "#         \"question\": \"일주지앤에스가 KNN 인물포커스에 소개된 날짜는 언제인가요?\",\n",
    "#         \"answers\": {\"text\": [\"2024년 5월 29일\"], \"answer_start\": [8]}\n",
    "#     },\n",
    "#     {\n",
    "#         \"context\": \"일주지앤에스는 태양광 발전소 조각투자 플랫폼 '햇나'를 런칭하여 블록체인 기반의 조각투자 서비스를 제공하고 있습니다.\",\n",
    "#         \"question\": \"일주지앤에스가 런칭한 태양광 발전소 조각투자 플랫폼의 이름은 무엇인가요?\",\n",
    "#         \"answers\": {\"text\": [\"햇나\"], \"answer_start\": [25]}\n",
    "#     },\n",
    "#     {\n",
    "#         \"context\": \"일주지앤에스는 미래기술연구소, DT사업부, IoT사업부, ESG사업부, SDM실, 제조ICT사업부, 대외사업부, E플랫폼사업부, 인프라사업부, 전략기획실, 경영지원실, 기술영업실로 구성되어 있습니다.\",\n",
    "#         \"question\": \"일주지앤에스의 조직 구성은 어떻게 되어 있나요?\",\n",
    "#         \"answers\": {\"text\": [\"미래기술연구소, DT사업부, IoT사업부, ESG사업부, SDM실, 제조ICT사업부, 대외사업부, E플랫폼사업부, 인프라사업부, 전략기획실, 경영지원실, 기술영업실\"], \"answer_start\": [8]}\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# # Hugging Face Dataset 객체로 변환\n",
    "# dataset = Dataset.from_list(custom_data)\n",
    "\n",
    "# # 데이터 전처리: 질문과 컨텍스트를 입력으로, 답변 텍스트를 라벨로 설정\n",
    "# def preprocess_data(examples):\n",
    "#     inputs = tokenizer(\n",
    "#         examples[\"question\"],\n",
    "#         examples[\"context\"],\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#         max_length=512\n",
    "#     )\n",
    "#     # 답변 텍스트를 라벨로 변환 (토큰 ID 형태)\n",
    "#     inputs[\"labels\"] = tokenizer(\n",
    "#         examples[\"answers\"][\"text\"][0],\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#         max_length=512\n",
    "#     )[\"input_ids\"]\n",
    "#     return inputs\n",
    "\n",
    "# full_tuning_dataset = dataset.map(preprocess_data)\n",
    "\n",
    "# # 학습 하이퍼파라미터 설정\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=new_model_path,             # 저장 경로\n",
    "#     per_device_train_batch_size=1,\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     num_train_epochs=10,\n",
    "#     logging_steps=1,\n",
    "#     save_total_limit=2,\n",
    "#     save_steps=1,\n",
    "#     eval_strategy=\"steps\",\n",
    "#     load_best_model_at_end=True,\n",
    "#     fp16=False,                            \n",
    "#     bf16=True\n",
    "# )\n",
    "\n",
    "# # Trainer 설정 (전체 모델의 파라미터 업데이트)\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=full_tuning_dataset,\n",
    "#     eval_dataset=full_tuning_dataset,\n",
    "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=100)] \n",
    "# )\n",
    "\n",
    "# print(\"Full tuning 시작...\")\n",
    "# trainer.train()\n",
    "# print(\"Full tuning 완료!\")\n",
    "\n",
    "# # 학습된 전체 모델 저장\n",
    "# trainer.save_model(new_model_path)\n",
    "# model.save_pretrained(new_model_path)\n",
    "# tokenizer.save_pretrained(new_model_path)\n",
    "\n",
    "# print(f\"Full tuned 모델이 {new_model_path}에 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sanguk-WEz904Hl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
