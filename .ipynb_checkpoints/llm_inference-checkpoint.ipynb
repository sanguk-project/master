{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b83532b5-c220-4b21-9524-00fa0298c1b9",
   "metadata": {},
   "source": [
    "# LangChain 기반 추론 및 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d2dc5-e5b9-4efe-8eab-c2334a319a9b",
   "metadata": {},
   "source": [
    "## 추론 테스트 및 Response 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "618b6912-6236-4eb5-9ec2-363635e4e9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `llm` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `llm`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4fc7f59-ea1c-49de-8431-00d6552bfc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b42cd85f2b4785a3c22e9ffaafbbdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63555a57c794eee9daf337b0dd73995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|█████████████████████████| 1000/1000 [20:08<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time for generating all responses: 1208.08 seconds\n",
      "\n",
      "Number of responses generated: 1000\n",
      "\n",
      "Total Unique Categories with Examples: 2\n",
      "\n",
      "=================================================================================================================================================\n",
      "SWC-107 - Example\n",
      "--------------------\n",
      "Instruction:\n",
      "Identify the vulnerability in this smart contract function and provide a revised version:\n",
      "\n",
      " 'function checkIfNameValid(string _nameStr) public view returns(bool) { bytes32 _name = _nameStr.nameFilter(); if (pIDxName_[_name] == 0) return (true); else return (false); }'\n",
      "\n",
      "Response:\n",
      "The given code is vulnerable to reentrancy attack, which can be fixed by adding an additional modifier that checks for non-reentrant calls before executing any other functions or statements within it - such as \"nonReentrant\" from OpenZeppelin Contracts library; this will ensure that only one transaction at once has access into our system while also preventing malicious actors who might try exploiting vulnerabilities through multiple concurrent transactions\n",
      "\n",
      "=================================================================================================================================================\n",
      "SWC-101 - Example\n",
      "--------------------\n",
      "Instruction:\n",
      "In the line `assert(sit.totalSupplyAt(initializedBlock) * 5 <= _totalSupplyCap);`, how can we mitigate potential overflow issues?\n",
      "\n",
      "Response:\n",
      "The SafeMath library is used to handle the arithmetic operations in Solidity, which includes a function called safeMul() that ensures no overflows occur when multiplying two numbers together (i-e., it returns zero if an error occurs). Therefore by using this library and calling its functions instead of directly performing math operations on variables or constants within our smart contract codebase itself; we ensure there are no possible errors due to integer underflow/overflows while still being able write concisely readable lines of codes!\n",
      "\n",
      "=================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import re\n",
    "import time\n",
    "from langchain import HuggingFacePipeline, LLMChain, PromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Hugging Face API token\n",
    "hf_token = \"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\"\n",
    "\n",
    "# Model and tokenizer path\n",
    "model_id = \"ukparkk/gemma-7b-r16-master\"\n",
    "# model_id = \"ukparkk/gemma-2b-r16-master\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=hf_token\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "# Set up text generation pipeline\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.3,\n",
    "    do_sample=True,\n",
    "    top_k=5,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "# Initialize LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "# Define LangChain Prompt Template\n",
    "prompt_template = PromptTemplate(input_variables=[\"instruction\"],\n",
    "                                 template=\"Instruction: {instruction}\\n\\nResponse:\")\n",
    "\n",
    "# Create LangChain LLMChain with the prompt and model\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Load test dataset\n",
    "test_data_path = '/workspace/dataset/test_data.jsonl'\n",
    "instructions = []\n",
    "\n",
    "with jsonlines.open(test_data_path) as reader:\n",
    "    for obj in reader:\n",
    "        instructions.append(obj['instruction'])\n",
    "\n",
    "# Prepare dataset with prompts for text generation\n",
    "dataset = Dataset.from_dict({\"instruction\": instructions})\n",
    "dataset = dataset.map(lambda x: {\"prompt\": f\"Instruction: {x['instruction']}\\n\\nResponse:\"})\n",
    "\n",
    "# SWC category terms\n",
    "category_patterns = {\n",
    "    \"SWC-101\": [\"swc-101\", \"integer overflow\", \"integer underflow\"],\n",
    "    \"SWC-107\": [\"swc-107\", \"reentrancy\"],\n",
    "    \"SWC-110\": [\"swc-110\", \"assert violation\"],\n",
    "    \"SWC-113\": [\"swc-113\", \"dos with failed call\"],\n",
    "    \"SWC-114\": [\"swc-114\", \"transaction order dependence\"]\n",
    "}\n",
    "\n",
    "# Generate responses for each instruction and store one response per category\n",
    "responses = []  # List to store all generated responses\n",
    "categories = {}  # Dictionary to store one response per category\n",
    "\n",
    "# Start the total execution time\n",
    "total_start_time = time.time()\n",
    "\n",
    "for instruction in tqdm(instructions, desc=\"Generating responses\"):\n",
    "    # Generate response using LangChain's LLMChain\n",
    "    generated_response = llm_chain.run(instruction=instruction)\n",
    "    response = generated_response.split(\"Response:\")[-1].strip()\n",
    "    \n",
    "    # Append the response to the responses list\n",
    "    responses.append(response)\n",
    "    \n",
    "    # Check for each category pattern in response\n",
    "    for category, terms in category_patterns.items():\n",
    "        if any(re.search(term, response, re.IGNORECASE) for term in terms):\n",
    "            # Only store the first response for each category\n",
    "            if category not in categories:\n",
    "                categories[category] = (instruction, response)  # Store instruction and response for this category\n",
    "\n",
    "# End the total execution time\n",
    "total_end_time = time.time()\n",
    "print(f\"\\nTotal time for generating all responses: {total_end_time - total_start_time:.2f} seconds\")\n",
    "\n",
    "# Print the total number of responses generated\n",
    "print(f\"\\nNumber of responses generated: {len(responses)}\")\n",
    "\n",
    "# Print the total number of unique categories with examples\n",
    "print(f\"\\nTotal Unique Categories with Examples: {len(categories)}\\n\")\n",
    "print(\"=\"*145)\n",
    "\n",
    "# Print an example response for each category with improved readability\n",
    "for category, (instruction, response) in categories.items():\n",
    "    print(f\"{category} - Example\")\n",
    "    print(\"-\" *20)\n",
    "    print(f\"Instruction:\\n{instruction}\\n\")\n",
    "    print(f\"Response:\\n{response}\\n\")\n",
    "    print(\"=\"*145)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b57b7c-66d1-409d-bc83-96a2fb7dee03",
   "metadata": {},
   "source": [
    "## F1-Score 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd507d2-8e99-4c88-a6b8-d8c31a2e6b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.3345\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 카테고리별 키워드 패턴 정의\n",
    "category_patterns = {\n",
    "    \"swc-101\": [\"swc-101\", \"integer overflow\", \"integer underflow\"],\n",
    "    \"swc-107\": [\"swc-107\", \"reentrancy\"],\n",
    "    \"swc-110\": [\"swc-110\", \"assert violation\"],\n",
    "    \"swc-113\": [\"swc-113\", \"dos with failed call\"],\n",
    "    \"swc-114\": [\"swc-114\", \"transaction order dependence\"]\n",
    "}\n",
    "\n",
    "# 각 카테고리의 키워드를 포함하는지 확인하고 해당하는 SWC를 예측\n",
    "def predict_category(response):\n",
    "    response_lower = response.lower()  # 대소문자 구분 없이 매칭하기 위해 소문자로 변환\n",
    "    for category, keywords in category_patterns.items():\n",
    "        if any(re.search(keyword, response_lower) for keyword in keywords):\n",
    "            return category  # 첫 번째로 매칭되는 카테고리를 반환\n",
    "    return \"unknown\"  # 아무런 매칭이 없을 경우\n",
    "\n",
    "# test_data.jsonl 파일 불러오기\n",
    "test_data_path = '/workspace/dataset/test_data.jsonl'\n",
    "ground_truth = []\n",
    "predictions = []\n",
    "\n",
    "with jsonlines.open(test_data_path) as reader:\n",
    "    for idx, obj in enumerate(reader):\n",
    "        # 실제 카테고리 정답 추가\n",
    "        ground_truth.append(obj['category'].lower())\n",
    "\n",
    "        # 예측된 response에서 카테고리 추출\n",
    "        response = responses[idx]  # 생성된 response 리스트에서 해당 응답 가져오기\n",
    "        predicted_category = predict_category(response)\n",
    "        predictions.append(predicted_category)\n",
    "\n",
    "# F1-score 계산\n",
    "f1 = f1_score(ground_truth, predictions, average='weighted')\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8fc059-4064-4f28-aa0f-577f78eaf48f",
   "metadata": {},
   "source": [
    "## Cosine Similarity 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c84dc452-fab3-4cb3-b7fd-0b3ec769a094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ground truth responses: 1000\n",
      "Number of generated responses: 1000\n",
      "Total pairs for cosine similarity calculation: 1000\n",
      "Average Cosine Similarity: 0.1057\n",
      "\n",
      "Example Cosine Similarities:\n",
      "Example 1: 0.1027\n",
      "Example 2: 0.2985\n",
      "Example 3: 0.1230\n",
      "Example 4: 0.1172\n",
      "Example 5: 0.1117\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load generated and ground truth responses\n",
    "test_data_path = '/workspace/dataset/test_data.jsonl'\n",
    "ground_truth_responses = []\n",
    "\n",
    "with jsonlines.open(test_data_path) as reader:\n",
    "    for obj in reader:\n",
    "        ground_truth_responses.append(obj['response'])\n",
    "\n",
    "# Assuming `responses` is your list of generated responses\n",
    "# Check if the number of generated responses matches the ground truth responses\n",
    "print(f\"Number of ground truth responses: {len(ground_truth_responses)}\")\n",
    "print(f\"Number of generated responses: {len(responses)}\")\n",
    "\n",
    "if len(responses) != len(ground_truth_responses):\n",
    "    print(\"Warning: The number of generated responses does not match the ground truth responses.\")\n",
    "else:\n",
    "    print(f\"Total pairs for cosine similarity calculation: {len(ground_truth_responses)}\")\n",
    "    \n",
    "    # Calculate cosine similarity between generated and ground truth responses\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    combined_responses = ground_truth_responses + responses\n",
    "    tfidf_matrix = vectorizer.fit_transform(combined_responses)\n",
    "    \n",
    "    # Split the matrix into ground truth and generated response vectors\n",
    "    ground_truth_vectors = tfidf_matrix[:len(ground_truth_responses)]\n",
    "    generated_vectors = tfidf_matrix[len(ground_truth_responses):]\n",
    "\n",
    "    # Calculate cosine similarities for each pair\n",
    "    similarities = [\n",
    "        cosine_similarity(ground_truth_vectors[i], generated_vectors[i])[0][0]\n",
    "        for i in range(len(ground_truth_responses))\n",
    "    ]\n",
    "    \n",
    "    # Output average cosine similarity and a few example similarities\n",
    "    average_similarity = sum(similarities) / len(similarities)\n",
    "    print(f\"Average Cosine Similarity: {average_similarity:.4f}\")\n",
    "    \n",
    "    # Display example similarities\n",
    "    print(\"\\nExample Cosine Similarities:\")\n",
    "    for i in range(min(5, len(similarities))):  # Show up to 5 examples\n",
    "        print(f\"Example {i + 1}: {similarities[i]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
