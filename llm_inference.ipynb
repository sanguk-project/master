{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b83532b5-c220-4b21-9524-00fa0298c1b9",
   "metadata": {},
   "source": [
    "# LangChain 기반 추론 및 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d2dc5-e5b9-4efe-8eab-c2334a319a9b",
   "metadata": {},
   "source": [
    "## 추론 테스트 및 Response 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "618b6912-6236-4eb5-9ec2-363635e4e9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `llm` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `llm`\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace 로그인 - 실제 사용 시 본인의 토큰으로 교체하세요\n",
    "# !huggingface-cli login --token YOUR_HF_TOKEN\n",
    "print(\"⚠️ 보안상의 이유로 실제 토큰이 제거되었습니다. 본인의 HuggingFace 토큰을 사용하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4fc7f59-ea1c-49de-8431-00d6552bfc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561caafafbe145169f5859d6c02c3f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/697 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89462c37446e4515a00f4a71b36075b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a90d6f353247e3b4c552522817a75c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/38.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac3148667164f4bbbc65b566478d954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15b297752e444d19617aa67b7d5eadc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ab80fa29c04ad294a7ac8391fd7533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/522 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b3bedf1b7140328379f4e3c782ac7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|█████████████████████████| 1000/1000 [54:32<00:00,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time for generating all responses: 3272.13 seconds\n",
      "\n",
      "Number of responses generated: 1000\n",
      "\n",
      "Total Unique Categories with Examples: 5\n",
      "\n",
      "=================================================================================================================================================\n",
      "SWC-101 - Example\n",
      "--------------------\n",
      "Instruction:\n",
      "Please explain the vulnerability in the following code snippet and provide a corrected version:\n",
      "\n",
      " 'function updateStartPhase(uint256 _time) public onlyOwner { startPase = _time; pase1 = startPase + 1 * 30 days; pase2 = pase1 + 1 * 30 days; pase3 = pase2 + 1 * 30 days; emit StartPaseUpdated(_time); }'\n",
      "\n",
      "Response:\n",
      "The code snippet has potential issues with integer overflow when calculating future timestamps for phases using time-based arithmetic operations. To mitigate this, it’s important to use SafeMath for all arithmatic operators involving timestaphes (e.g., adding or subtracting). The corrected code is as follows:\n",
      "\n",
      "```solidity\n",
      " function updatedStarPate()public{  //update phase1 & fase2 &...   }\n",
      " ```\n",
      "This ensures that any potentially risky calculations are handled safely without overflow concerns. For more complex timestamp math, consider utilizing OpenZeppelin's `Deadline` library which includes checks for expired dates etc.. Overall, while this issue isn't directly related to SWC-101, best practices around safe date/timestamp handling should be applied moving forward.\n",
      "\n",
      "Note: This answer assumes Solidity versions >= v0.8 where basic overflows aren‘t an issue but best practice should still apply regarding dated functions.\n",
      "\n",
      "=================================================================================================================================================\n",
      "SWC-113 - Example\n",
      "--------------------\n",
      "Instruction:\n",
      "Examine the following code snippet and identify potential issues in its logic:\n",
      "\n",
      "function StartGame(string _question,string _response) public payable {\n",
      "        if(responseHash==0x0) {\n",
      "            responseHash = keccak256(_response);\n",
      "            question = _question;\n",
      "            questionSender = msg.sender;\n",
      "        }\n",
      "    }\n",
      "\n",
      "Response:\n",
      "To enhance security and prevent manipulation of the `_response`, an additional check should be added to ensure that the function is only executed once by the sender if necessary. The corrected version of this line can be as follows:\n",
      "\n",
      "```solidity\n",
      "require(!isExecuted(), 'Function already executed'); // Prevent further executions\n",
      "// Original code...\n",
      "``\n",
      "\n",
      "This ensures that the function cannot be re-executed after it has been completed (or prevents unintended actions), addressing the SWC-113 vulnerability related to improper state management during initialization or repeated execution. SWC-147 also suggests ensuring unique identifiers for such states so they cannot easily be altered under attack.\n",
      "\n",
      "=================================================================================================================================================\n",
      "SWC-110 - Example\n",
      "--------------------\n",
      "Instruction:\n",
      "Consider the following Solidity code: 'function addresses(string name) public view returns(address) { return _addresses[stringToBytes32(name)]; }'. What issues might be present in this code?\n",
      "\n",
      "Response:\n",
      "The function assumes that a mapping of bytes32 keys does not exceed 32 characters, which can lead to unexpected behavior if larger strings are provided. A safer implementation would ensure proper handling of longer names by using an additional check or reverting on overflow. The corrected version could be: '_mappingLength = stringSize(_data); require (_length <=  bytes3).length;...' where appropriate logic is implemented based on the size of the input data. This resolves the assert violation as per SWC-101 guidelines.\n",
      "\n",
      "=================================================================================================================================================\n",
      "SWC-107 - Example\n",
      "--------------------\n",
      "Instruction:\n",
      "Consider the following Solidity code: 'function subBalances(address[] recipients, uint256[] moenys) public onlyOwner { uint256 sum = 0; for(uint256 i = 0; i < recipients.length; i++) { balances[recipients[i]] = balances[recipients[i]].sub(moenys[i]); sum = sum.add(moenys[i]); emit Transfer(recipients[i], this, moenys[i]); } balances[this] = balances[this].add(sum); sysusermoney = sysusermoney.add(sum); }'. What issues might be present in this code?\n",
      "\n",
      "Response:\n",
      "The line '(balances[_to])', where '_to' is a recipient address, can cause an overflow if too many tokens are transferred, resulting in incorrect balance calculations and potential vulnerabilities to reentrancy attacks (SWC-101). To fix it, ensure that proper checks on token amounts are made before subtracting them from individual accounts or adding new values to the contract’s balance – always use SafeMath functions like '.safeSub()' when possible.'\n",
      "\n",
      "=================================================================================================================================================\n",
      "SWC-114 - Example\n",
      "--------------------\n",
      "Instruction:\n",
      "Analyze the `depositToken` function for potential reentrancy vulnerabilities:\n",
      "\n",
      "function depositToken(address _token, uint _amount) deprecable {\n",
      "    require(_token != 0);\n",
      "    if (!Token(_token).transferFrom(msg.sender, this, _amount)) {\n",
      "      revert();\n",
      "    }\n",
      "    tokens[_token][msg.sender] = safeAdd(tokens[_token][msg.sender], _amount);\n",
      "    Deposit(_token, msg.sender, _amount, tokens[_token][msg.sender]);\n",
      "}\n",
      "\n",
      "Response:\n",
      "The 'dependant calls' vulnerability described in SWC-114 can occur during the `positedTokens()» call after updating `balances`. If an external contract could be called before completing all state changes, it might exploit the newly updated balance. To mitigate this issue, you should update balances and states before invoking any potentially reentrant functions like so`:\n",
      "\n",
      "function depositToken ( address _token,  uint256 _Amount ) public payable{ require (_ token == address ( zeroAddress )) ; // Check if the token is valid tokens [ _ ] [ msg. sender ] [_ Amount + _ amount ]; / / Update the balance first to avoid reentrance issues Tokens [ ceoAddress ][_ Token ]. transferfrom ^( messageSender ^), ^(this ^)，^(-_ Amount )); emit Deposit （[]=$,_ source Address，=balance[CEOCEO]=safeadd（Balance［ CEO CE O］ ，=_ Amt ）; }\n",
      "\n",
      "=================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import re\n",
    "import time\n",
    "from langchain import HuggingFacePipeline, LLMChain, PromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Hugging Face API token - 실제 사용 시 환경변수로 설정하세요\n",
    "# hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "hf_token = \"YOUR_HF_TOKEN\"  # 실제 토큰으로 교체하세요\n",
    "\n",
    "# Model and tokenizer path\n",
    "model_id = \"ukparkk/gemma-7b-r16-master\"\n",
    "# model_id = \"ukparkk/gemma-2b-r16-master\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=hf_token\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "# Set up text generation pipeline\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.3,\n",
    "    do_sample=True,\n",
    "    top_k=5,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "# Initialize LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "# Define LangChain Prompt Template\n",
    "prompt_template = PromptTemplate(input_variables=[\"instruction\"],\n",
    "                                 template=\"Instruction: {instruction}\\n\\nResponse:\")\n",
    "\n",
    "# Create LangChain LLMChain with the prompt and model\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Load test dataset\n",
    "test_data_path = '/workspace/dataset/test_data.jsonl'\n",
    "instructions = []\n",
    "\n",
    "with jsonlines.open(test_data_path) as reader:\n",
    "    for obj in reader:\n",
    "        instructions.append(obj['instruction'])\n",
    "\n",
    "# Prepare dataset with prompts for text generation\n",
    "dataset = Dataset.from_dict({\"instruction\": instructions})\n",
    "dataset = dataset.map(lambda x: {\"prompt\": f\"Instruction: {x['instruction']}\\n\\nResponse:\"})\n",
    "\n",
    "# SWC category terms\n",
    "category_patterns = {\n",
    "    \"SWC-101\": [\"swc-101\", \"integer overflow\", \"integer underflow\"],\n",
    "    \"SWC-107\": [\"swc-107\", \"reentrancy\"],\n",
    "    \"SWC-110\": [\"swc-110\", \"assert violation\"],\n",
    "    \"SWC-113\": [\"swc-113\", \"dos with failed call\"],\n",
    "    \"SWC-114\": [\"swc-114\", \"transaction order dependence\"]\n",
    "}\n",
    "\n",
    "# Generate responses for each instruction and store one response per category\n",
    "responses = []  # List to store all generated responses\n",
    "categories = {}  # Dictionary to store one response per category\n",
    "\n",
    "# Start the total execution time\n",
    "total_start_time = time.time()\n",
    "\n",
    "for instruction in tqdm(instructions, desc=\"Generating responses\"):\n",
    "    # Generate response using LangChain's LLMChain\n",
    "    generated_response = llm_chain.run(instruction=instruction)\n",
    "    response = generated_response.split(\"Response:\")[-1].strip()\n",
    "    \n",
    "    # Append the response to the responses list\n",
    "    responses.append(response)\n",
    "    \n",
    "    # Check for each category pattern in response\n",
    "    for category, terms in category_patterns.items():\n",
    "        if any(re.search(term, response, re.IGNORECASE) for term in terms):\n",
    "            # Only store the first response for each category\n",
    "            if category not in categories:\n",
    "                categories[category] = (instruction, response)  # Store instruction and response for this category\n",
    "\n",
    "# End the total execution time\n",
    "total_end_time = time.time()\n",
    "print(f\"\\nTotal time for generating all responses: {total_end_time - total_start_time:.2f} seconds\")\n",
    "\n",
    "# Print the total number of responses generated\n",
    "print(f\"\\nNumber of responses generated: {len(responses)}\")\n",
    "\n",
    "# Print the total number of unique categories with examples\n",
    "print(f\"\\nTotal Unique Categories with Examples: {len(categories)}\\n\")\n",
    "print(\"=\"*145)\n",
    "\n",
    "# Print an example response for each category with improved readability\n",
    "for category, (instruction, response) in categories.items():\n",
    "    print(f\"{category} - Example\")\n",
    "    print(\"-\" *20)\n",
    "    print(f\"Instruction:\\n{instruction}\\n\")\n",
    "    print(f\"Response:\\n{response}\\n\")\n",
    "    print(\"=\"*145)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b57b7c-66d1-409d-bc83-96a2fb7dee03",
   "metadata": {},
   "source": [
    "## F1-Score 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd507d2-8e99-4c88-a6b8-d8c31a2e6b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.3345\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 카테고리별 키워드 패턴 정의\n",
    "category_patterns = {\n",
    "    \"swc-101\": [\"swc-101\", \"integer overflow\", \"integer underflow\"],\n",
    "    \"swc-107\": [\"swc-107\", \"reentrancy\"],\n",
    "    \"swc-110\": [\"swc-110\", \"assert violation\"],\n",
    "    \"swc-113\": [\"swc-113\", \"dos with failed call\"],\n",
    "    \"swc-114\": [\"swc-114\", \"transaction order dependence\"]\n",
    "}\n",
    "\n",
    "# 각 카테고리의 키워드를 포함하는지 확인하고 해당하는 SWC를 예측\n",
    "def predict_category(response):\n",
    "    response_lower = response.lower()  # 대소문자 구분 없이 매칭하기 위해 소문자로 변환\n",
    "    for category, keywords in category_patterns.items():\n",
    "        if any(re.search(keyword, response_lower) for keyword in keywords):\n",
    "            return category  # 첫 번째로 매칭되는 카테고리를 반환\n",
    "    return \"unknown\"  # 아무런 매칭이 없을 경우\n",
    "\n",
    "# test_data.jsonl 파일 불러오기\n",
    "test_data_path = '/workspace/dataset/test_data.jsonl'\n",
    "ground_truth = []\n",
    "predictions = []\n",
    "\n",
    "with jsonlines.open(test_data_path) as reader:\n",
    "    for idx, obj in enumerate(reader):\n",
    "        # 실제 카테고리 정답 추가\n",
    "        ground_truth.append(obj['category'].lower())\n",
    "\n",
    "        # 예측된 response에서 카테고리 추출\n",
    "        response = responses[idx]  # 생성된 response 리스트에서 해당 응답 가져오기\n",
    "        predicted_category = predict_category(response)\n",
    "        predictions.append(predicted_category)\n",
    "\n",
    "# F1-score 계산\n",
    "f1 = f1_score(ground_truth, predictions, average='weighted')\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8fc059-4064-4f28-aa0f-577f78eaf48f",
   "metadata": {},
   "source": [
    "## Cosine Similarity 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c84dc452-fab3-4cb3-b7fd-0b3ec769a094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ground truth responses: 1000\n",
      "Number of generated responses: 1000\n",
      "Total pairs for cosine similarity calculation: 1000\n",
      "Average Cosine Similarity: 0.1057\n",
      "\n",
      "Example Cosine Similarities:\n",
      "Example 1: 0.1027\n",
      "Example 2: 0.2985\n",
      "Example 3: 0.1230\n",
      "Example 4: 0.1172\n",
      "Example 5: 0.1117\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load generated and ground truth responses\n",
    "test_data_path = '/workspace/dataset/test_data.jsonl'\n",
    "ground_truth_responses = []\n",
    "\n",
    "with jsonlines.open(test_data_path) as reader:\n",
    "    for obj in reader:\n",
    "        ground_truth_responses.append(obj['response'])\n",
    "\n",
    "# Assuming `responses` is your list of generated responses\n",
    "# Check if the number of generated responses matches the ground truth responses\n",
    "print(f\"Number of ground truth responses: {len(ground_truth_responses)}\")\n",
    "print(f\"Number of generated responses: {len(responses)}\")\n",
    "\n",
    "if len(responses) != len(ground_truth_responses):\n",
    "    print(\"Warning: The number of generated responses does not match the ground truth responses.\")\n",
    "else:\n",
    "    print(f\"Total pairs for cosine similarity calculation: {len(ground_truth_responses)}\")\n",
    "    \n",
    "    # Calculate cosine similarity between generated and ground truth responses\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    combined_responses = ground_truth_responses + responses\n",
    "    tfidf_matrix = vectorizer.fit_transform(combined_responses)\n",
    "    \n",
    "    # Split the matrix into ground truth and generated response vectors\n",
    "    ground_truth_vectors = tfidf_matrix[:len(ground_truth_responses)]\n",
    "    generated_vectors = tfidf_matrix[len(ground_truth_responses):]\n",
    "\n",
    "    # Calculate cosine similarities for each pair\n",
    "    similarities = [\n",
    "        cosine_similarity(ground_truth_vectors[i], generated_vectors[i])[0][0]\n",
    "        for i in range(len(ground_truth_responses))\n",
    "    ]\n",
    "    \n",
    "    # Output average cosine similarity and a few example similarities\n",
    "    average_similarity = sum(similarities) / len(similarities)\n",
    "    print(f\"Average Cosine Similarity: {average_similarity:.4f}\")\n",
    "    \n",
    "    # Display example similarities\n",
    "    print(\"\\nExample Cosine Similarities:\")\n",
    "    for i in range(min(5, len(similarities))):  # Show up to 5 examples\n",
    "        print(f\"Example {i + 1}: {similarities[i]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
