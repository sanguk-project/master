# LLM νμΈνλ‹ ν”„λ΅μ νΈ (Large Language Model Fine-tuning)

λ³Έ ν”„λ΅μ νΈλ” λ‹¤μ–‘ν• λ€κ·λ¨ μ–Έμ–΄ λ¨λΈ(LLM)μ„ νμΈνλ‹ν•μ—¬ νΉμ • μ‘μ—…μ— λ§κ² μ»¤μ¤ν„°λ§μ΄μ§•ν•λ” μ‹¤ν—μ  μ—°κµ¬ ν”„λ΅μ νΈμ…λ‹λ‹¤.

## π― ν”„λ΅μ νΈ κ°μ”

μ΄ λ¦¬ν¬μ§€ν† λ¦¬λ” λ‹¤μκ³Ό κ°™μ€ LLM λ¨λΈλ“¤μ— λ€ν• νμΈνλ‹ μ‹¤ν—μ„ ν¬ν•¨ν•κ³  μμµλ‹λ‹¤:
- **Google Gemma** μ‹λ¦¬μ¦ (2B, 7B)
- **Meta LLaMA** μ‹λ¦¬μ¦ (3.2B Korean Bllossom)
- **CodeGemma** (μ½”λ“ νΉν™” λ¨λΈ)

## π“ ν”„λ΅μ νΈ κµ¬μ΅°

```
β”β”€β”€ dataset/                        # ν›λ ¨ λ°μ΄ν„°μ…‹ μ €μ¥μ†
β”‚   β”β”€β”€ train_data.jsonl            # ν›λ ¨μ© λ©”μΈ λ°μ΄ν„°μ…‹ (4.7MB)
β”‚   β”β”€β”€ test_data.jsonl             # ν…μ¤νΈμ© λ°μ΄ν„°μ…‹ (1.2MB)
β”‚   β”β”€β”€ solidity_vunerability_SWC-101_*.jsonl  # μ •μ μ¤λ²„ν”λ΅μ° μ·¨μ•½μ 
β”‚   β”β”€β”€ solidity_vunerability_SWC-107_*.jsonl  # μ¬μ§„μ… κ³µκ²© μ·¨μ•½μ 
β”‚   β”β”€β”€ solidity_vunerability_SWC-110_*.jsonl  # Assert μ„λ° μ·¨μ•½μ 
β”‚   β”β”€β”€ solidity_vunerability_SWC-113_*.jsonl  # DoS κ³µκ²© μ·¨μ•½μ 
β”‚   β””β”€β”€ solidity_vunerability_SWC-114_*.jsonl  # νΈλμ­μ… μμ„ μμ΅΄ μ·¨μ•½μ 
β”β”€β”€ model/                          # ν•™μµλ λ¨λΈ μ €μ¥μ†
β”‚   β”β”€β”€ gemma-7b-r{8,16}/           # Gemma 7B νμΈνλ‹ λ¨λΈλ“¤
β”‚   β”β”€β”€ codegemma-7b-r{8,16}/       # CodeGemma 7B νμΈνλ‹ λ¨λΈλ“¤
β”‚   β””β”€β”€ runs/                       # ν•™μµ λ΅κ·Έ λ° μ²΄ν¬ν¬μΈνΈ
β”β”€β”€ gemma_fine-tuning.ipynb         # Gemma λ¨λΈ νμΈνλ‹ (λ©”μΈ)
β”β”€β”€ gemma_2b_fine_tuning_l4.ipynb   # Gemma 2B L4 νμΈνλ‹
β”β”€β”€ gemma_7b_fine_tuning_fin.ipynb  # Gemma 7B μµμΆ… νμΈνλ‹
β”β”€β”€ llama_fine_tuning.ipynb         # LLaMA λ¨λΈ νμΈνλ‹
β”β”€β”€ learning_llm_test.ipynb         # LLM ν•™μµ ν…μ¤νΈ
β”β”€β”€ llm_inference.ipynb             # λ¨λΈ μ¶”λ΅  λ° ν‰κ°€
β”β”€β”€ huggigface_model.ipynb          # Hugging Face λ¨λΈ μ—°λ™
β””β”€β”€ README.md                       # ν”„λ΅μ νΈ μ„¤λ…μ„
```

## π€ μ£Όμ” κΈ°λ¥

### 1. λ‹¤μ–‘ν• λ¨λΈ μ§€μ›
- **Gemma 2B/7B**: Googleμ κ²½λ‰ν™”λ μ–Έμ–΄λ¨λΈ
- **CodeGemma 7B**: μ½”λ“ μƒμ„±μ— νΉν™”λ λ¨λΈ
- **LLaMA 3.2 Korean Bllossom**: ν•κµ­μ–΄ νΉν™” λ¨λΈ

### 2. μµμ ν™” κΈ°λ²•
- **QLoRA (Quantized LoRA)**: λ©”λ¨λ¦¬ ν¨μ¨μ μΈ νμΈνλ‹
- **4λΉ„νΈ μ–‘μν™”**: BitsAndBytesConfigλ¥Ό ν™μ©ν• λ©”λ¨λ¦¬ μµμ ν™”
- **Flash Attention 2**: H100κ³Ό κ°™μ€ μµμ‹  GPUμ—μ„ μ„±λ¥ μµμ ν™”

### 3. νΉν™”λ λ°μ΄ν„°μ…‹
- **Solidity μ·¨μ•½μ  λ¶„μ„**: 5κ°€μ§€ SWC λ¶„λ¥ κΈ°λ° μ¤λ§νΈ μ»¨νΈλ™νΈ λ³΄μ• μ·¨μ•½μ  νƒμ§€
- **ν›λ ¨/ν…μ¤νΈ λ°μ΄ν„°**: μ§€λ„ν•™μµμ© instruction-response μ
- **μ½”λ“ λ³΄μ• λ¶„μ„**: μ·¨μ•½μ  μ‹λ³„ λ° μ•μ „ν• μ½”λ“ μ μ•

## π› οΈ μ„¤μΉ λ° μ„¤μ •

### ν•„μ μ”κµ¬μ‚¬ν•­
```bash
# GPU ν™κ²½ (CUDA 11.8 μ΄μƒ)
# Python 3.8 μ΄μƒ
# μµμ† 16GB GPU λ©”λ¨λ¦¬ κ¶μ¥ (H100 ν™κ²½ μµμ ν™”)
```

### μμ΅΄μ„± μ„¤μΉ
```bash
# κΈ°λ³Έ λΌμ΄λΈλ¬λ¦¬
pip install torch torchvision torchaudio
pip install transformers datasets accelerate peft
pip install bitsandbytes

# Keras λ° JAX (Gemma μ©)
pip install tensorflow keras keras-nlp jax[cuda]

# Flash Attention (μ„ νƒμ‚¬ν•­ - H100 λ“± μµμ‹  GPU)
pip install flash-attn

# Hugging Face λ° Kaggle
pip install huggingface-hub kaggle
```

## π“ μ‚¬μ©λ²•

### 1. Gemma λ¨λΈ νμΈνλ‹
```python
# gemma_fine-tuning.ipynb μ‹¤ν–‰
# - dataset/train_data.jsonl λ° SWC μ·¨μ•½μ  λ°μ΄ν„°μ…‹ μ‚¬μ©
# - JAX/Keras ν”„λ μ„μ›ν¬ κΈ°λ°
# - CodeGemma 7B λ¨λΈ ν™μ©
# - μ¤λ§νΈ μ»¨νΈλ™νΈ μ·¨μ•½μ  λ¶„μ„μ— νΉν™”
```

### 2. LLaMA λ¨λΈ νμΈνλ‹
```python
# llama_fine_tuning.ipynb μ‹¤ν–‰
# - dataset/train_data.jsonl λ©”μΈ ν›λ ¨ λ°μ΄ν„°μ…‹ μ‚¬μ©
# - PyTorch/Transformers κΈ°λ°
# - QLoRA μµμ ν™” μ μ©
# - μΌλ°μ μΈ instruction-following νƒμ¤ν¬
```

### 3. λ¨λΈ μ¶”λ΅  λ° ν…μ¤νΈ
```python
# llm_inference.ipynb μ‹¤ν–‰
# - ν•™μµλ λ¨λΈ λ΅λ“ λ° μ¶”λ΅ 
# - dataset/test_data.jsonlλ΅ μ„±λ¥ ν‰κ°€
# - μ‹¤μ‹κ°„ μ·¨μ•½μ  λ¶„μ„ ν…μ¤νΈ
```

## π“ λ°μ΄ν„°μ…‹ μ •λ³΄

λ³Έ ν”„λ΅μ νΈμ—μ„ μ‚¬μ©λλ” μ£Όμ” λ°μ΄ν„°μ…‹μ€ `dataset/` ν΄λ”μ— μ €μ¥λμ–΄ μμµλ‹λ‹¤:

### 1. **ν›λ ¨ λ° ν…μ¤νΈ λ°μ΄ν„°μ…‹**
- **`train_data.jsonl`** (4.7MB): λ¨λΈ ν›λ ¨μ© λ©”μΈ λ°μ΄ν„°μ…‹
- **`test_data.jsonl`** (1.2MB): λ¨λΈ ν‰κ°€μ© ν…μ¤νΈ λ°μ΄ν„°μ…‹

### 2. **Solidity μ¤λ§νΈ μ»¨νΈλ™νΈ μ·¨μ•½μ  λ°μ΄ν„°μ…‹**
**SWC(Smart Contract Weakness Classification)** κΈ°λ°μ νΉμ • μ·¨μ•½μ  μ ν•λ³„ λ°μ΄ν„°μ…‹:

- **`solidity_vunerability_SWC-101_Integer_Overflow_and_Underflow.jsonl`** (1.2MB)
  - μ •μ μ¤λ²„ν”λ΅μ°/μ–Έλ”ν”λ΅μ° μ·¨μ•½μ  μμ‹
- **`solidity_vunerability_SWC-107_Reentrancy.jsonl`** (1.0MB) 
  - μ¬μ§„μ…(Reentrancy) κ³µκ²© μ·¨μ•½μ  μμ‹
- **`solidity_vunerability_SWC-110_Assert_Violation.jsonl`** (1.2MB)
  - Assert μ„λ° μ·¨μ•½μ  μμ‹
- **`solidity_vunerability_SWC-113_DoS_with_Failed_Call.jsonl`** (1.2MB)
  - μ‹¤ν¨ν• νΈμ¶λ΅ μΈν• μ„λΉ„μ¤ κ±°λ¶€ κ³µκ²© μ·¨μ•½μ  μμ‹
- **`solidity_vunerability_SWC-114_Transaction_Order_Dependence.jsonl`** (1.3MB)
  - νΈλμ­μ… μμ„ μμ΅΄μ„± μ·¨μ•½μ  μμ‹

### 3. **λ°μ΄ν„°μ…‹ κµ¬μ΅°**
κ° λ°μ΄ν„°μ…‹μ€ JSONL ν•μ‹μΌλ΅ κµ¬μ„±λλ©°, λ‹¤μκ³Ό κ°™μ€ ν•„λ“λ¥Ό ν¬ν•¨:
```json
{
  "instruction": "μ·¨μ•½ν• μ½”λ“μ— λ€ν• λ¶„μ„ μ”μ²­ λλ” λ¬Έμ  μ„¤λ…",
  "response": "λ³΄μ• λ¬Έμ μ  μ‹λ³„ λ° μμ •λ μ½”λ“ μ κ³µ", 
  "category": "SWC-XXX (ν•΄λ‹Ή μ·¨μ•½μ  λ¶„λ¥)"
}
```

### 4. **λ°μ΄ν„°μ…‹ νΉμ§•**
- **μ΄ λ°μ΄ν„° κ·λ¨**: μ•½ 11.8MB (6κ° νμΌ)
- **ν•μ‹**: Instruction-Response μμΌλ΅ κµ¬μ„±λ μ§€λ„ν•™μµμ© λ°μ΄ν„°
- **μ–Έμ–΄**: Solidity μ½”λ“ λ° μμ–΄ μ„¤λ…
- **μ·¨μ•½μ  μ ν•**: 5κ°€μ§€ μ£Όμ” μ¤λ§νΈ μ»¨νΈλ™νΈ μ·¨μ•½μ  λ¶„λ¥
- **ν™μ© λ©μ **: CodeGemma λ° Gemma λ¨λΈμ λ³΄μ• μ½”λ“ λ¶„μ„ λ¥λ ¥ ν–¥μƒ

### 5. **μ£Όμ” μ·¨μ•½μ  μ ν• μ„¤λ…**
- **SWC-101**: μ •μ μ—°μ‚° μ¤‘ λ°μƒν•λ” μ¤λ²„ν”λ΅μ°/μ–Έλ”ν”λ΅μ°
- **SWC-107**: μ¬μ§„μ… κ³µκ²©μΌλ΅ μΈν• μκΈ νƒμ·¨ μ„ν—
- **SWC-110**: Assert λ¬Έ μ„λ°μΌλ΅ μΈν• μ»¨νΈλ™νΈ μ‹¤ν–‰ μ¤‘λ‹¨
- **SWC-113**: μ™Έλ¶€ νΈμ¶ μ‹¤ν¨λ΅ μΈν• μ„λΉ„μ¤ κ±°λ¶€ κ³µκ²©
- **SWC-114**: νΈλμ­μ… μμ„μ— μμ΅΄ν•λ” λ΅μ§μ μ·¨μ•½μ 

## β™οΈ ν•μ΄νΌνλΌλ―Έν„° μ„¤μ •

### LoRA μ„¤μ •
```python
# μΌλ°μ μΈ LoRA μ„¤μ •
lora_config = LoraConfig(
    r=8,  # rank (r4, r8, r16 μ‹¤ν—)
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1
)
```

### 4λΉ„νΈ μ–‘μν™” μ„¤μ •
```python
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=False
)
```

## π“ λ¨λΈ μ„±λ¥

κ° λ¨λΈλ³„λ΅ λ‹¤μ–‘ν• rank μ„¤μ •(r4, r8, r16)μΌλ΅ μ‹¤ν—μ„ μ§„ν–‰ν•μ—¬ μµμ μ νλΌλ―Έν„°λ¥Ό νƒμƒ‰ν•©λ‹λ‹¤.

### μ €μ¥λ λ¨λΈλ“¤
- `gemma-7b-r8-master/`: Gemma 7B (rank 8)
- `codegemma-7b-r16-master/`: CodeGemma 7B (rank 16)
- κ° λ¨λΈμ— λ€ν•΄ instruction-tuning λ²„μ „λ„ λ³„λ„ μ €μ¥

## π–¥οΈ ν•λ“μ›¨μ–΄ μ”κµ¬μ‚¬ν•­

### κ¶μ¥ ν™κ²½
- **GPU**: NVIDIA H100 (80GB VRAM)
- **λ©”λ¨λ¦¬**: 32GB μ΄μƒ μ‹μ¤ν… RAM
- **μ €μ¥κ³µκ°„**: 100GB μ΄μƒ (λ¨λΈ λ° λ°μ΄ν„°μ…‹ μ €μ¥μ©)

### μµμ ν™” νΉμ§•
- Flash Attention 2 μ§€μ› (H100 ν™κ²½)
- Mixed precision training (bfloat16)
- Gradient checkpointing
- Multi-GPU μ§€μ› (device_map="auto")

## π”¬ μ‹¤ν— κ²°κ³Ό

κ° λ…ΈνΈλ¶μ—μ„ μν–‰ν• μ‹¤ν—λ“¤:
1. **λ¨λΈ ν¬κΈ°λ³„ μ„±λ¥ λΉ„κµ** (2B vs 7B)
2. **LoRA rank μµμ ν™”** (r4, r8, r16)
3. **μ–‘μν™” ν¨κ³Ό λ¶„μ„** (4bit vs full precision)
4. **λ„λ©”μΈλ³„ νΉν™”** (μ½”λ“ vs μΌλ° ν…μ¤νΈ vs ν•κµ­μ–΄)

## π“„ λΌμ΄μ„ μ¤

λ³Έ ν”„λ΅μ νΈλ” μ—°κµ¬ λ©μ μΌλ΅ μ μ‘λμ—μµλ‹λ‹¤. μ‚¬μ©ν•λ” λ¨λΈλ“¤μ λΌμ΄μ„ μ¤λ¥Ό μ¤€μν•΄μ£Όμ„Έμ”:
- Gemma: [Gemma Terms of Use](https://ai.google.dev/gemma/terms)
- LLaMA: [LLaMA 2 License](https://github.com/facebookresearch/llama/blob/main/LICENSE)

## π“ λ¬Έμμ‚¬ν•­

- ν”„λ΅μ νΈ κ΄€λ ¨ λ¬Έμμ‚¬ν•­μ΄ μμΌμ‹λ©΄ Issueλ¥Ό μƒμ„±ν•΄μ£Όμ„Έμ”.
- E-mail: parksu9997@gmail.com
---