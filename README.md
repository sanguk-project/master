# LLM νμΈνλ‹ ν”„λ΅μ νΈ (Large Language Model Fine-tuning)

λ³Έ ν”„λ΅μ νΈλ” λ‹¤μ–‘ν• λ€κ·λ¨ μ–Έμ–΄ λ¨λΈ(LLM)μ„ νμΈνλ‹ν•μ—¬ νΉμ • μ‘μ—…μ— λ§κ² μ»¤μ¤ν„°λ§μ΄μ§•ν•λ” μ‹¤ν—μ  μ—°κµ¬ ν”„λ΅μ νΈμ…λ‹λ‹¤.

## π― ν”„λ΅μ νΈ κ°μ”

μ΄ λ¦¬ν¬μ§€ν† λ¦¬λ” λ‹¤μκ³Ό κ°™μ€ LLM λ¨λΈλ“¤μ— λ€ν• νμΈνλ‹ μ‹¤ν—μ„ ν¬ν•¨ν•κ³  μμµλ‹λ‹¤:
- **Google Gemma** μ‹λ¦¬μ¦ (2B, 7B)
- **Meta LLaMA** μ‹λ¦¬μ¦ (3.2B Korean Bllossom)
- **CodeGemma** (μ½”λ“ νΉν™” λ¨λΈ)

## π“ ν”„λ΅μ νΈ κµ¬μ΅°

```
β”β”€β”€ model/                          # ν•™μµλ λ¨λΈ μ €μ¥μ†
β”‚   β”β”€β”€ gemma-7b-r{8,16}/           # Gemma 7B νμΈνλ‹ λ¨λΈλ“¤
β”‚   β”β”€β”€ codegemma-7b-r{8,16}/       # CodeGemma 7B νμΈνλ‹ λ¨λΈλ“¤
β”‚   β””β”€β”€ runs/                       # ν•™μµ λ΅κ·Έ λ° μ²΄ν¬ν¬μΈνΈ
β”β”€β”€ gemma_fine-tuning.ipynb         # Gemma λ¨λΈ νμΈνλ‹ (λ©”μΈ)
β”β”€β”€ gemma_2b_fine_tuning_l4.ipynb   # Gemma 2B L4 νμΈνλ‹
β”β”€β”€ gemma_7b_fine_tuning_fin.ipynb  # Gemma 7B μµμΆ… νμΈνλ‹
β”β”€β”€ llama_fine_tuning.ipynb         # LLaMA λ¨λΈ νμΈνλ‹
β”β”€β”€ learning_llm_test.ipynb         # LLM ν•™μµ ν…μ¤νΈ
β”β”€β”€ llm_inference.ipynb             # λ¨λΈ μ¶”λ΅  λ° ν‰κ°€
β””β”€β”€ huggigface_model.ipynb          # Hugging Face λ¨λΈ μ—°λ™
```

## π€ μ£Όμ” κΈ°λ¥

### 1. λ‹¤μ–‘ν• λ¨λΈ μ§€μ›
- **Gemma 2B/7B**: Googleμ κ²½λ‰ν™”λ μ–Έμ–΄λ¨λΈ
- **CodeGemma 7B**: μ½”λ“ μƒμ„±μ— νΉν™”λ λ¨λΈ
- **LLaMA 3.2 Korean Bllossom**: ν•κµ­μ–΄ νΉν™” λ¨λΈ

### 2. μµμ ν™” κΈ°λ²•
- **QLoRA (Quantized LoRA)**: λ©”λ¨λ¦¬ ν¨μ¨μ μΈ νμΈνλ‹
- **4λΉ„νΈ μ–‘μν™”**: BitsAndBytesConfigλ¥Ό ν™μ©ν• λ©”λ¨λ¦¬ μµμ ν™”
- **Flash Attention 2**: H100κ³Ό κ°™μ€ μµμ‹  GPUμ—μ„ μ„±λ¥ μµμ ν™”

### 3. νΉν™”λ λ°μ΄ν„°μ…‹
- **Solidity μ·¨μ•½μ  λ¶„μ„**: μ¤λ§νΈ μ»¨νΈλ™νΈ λ³΄μ• μ·¨μ•½μ  νƒμ§€
- **ν•κµ­μ–΄ QA**: μ§λ¬Έ-λ‹µλ³€ ν•νƒμ ν•κµ­μ–΄ λ°μ΄ν„°μ…‹
- **μ½”λ“ μƒμ„±**: ν”„λ΅κ·Έλλ° κ΄€λ ¨ instruction-following

## π› οΈ μ„¤μΉ λ° μ„¤μ •

### ν•„μ μ”κµ¬μ‚¬ν•­
```bash
# GPU ν™κ²½ (CUDA 11.8 μ΄μƒ)
# Python 3.8 μ΄μƒ
# μµμ† 16GB GPU λ©”λ¨λ¦¬ κ¶μ¥ (H100 ν™κ²½ μµμ ν™”)
```

### μμ΅΄μ„± μ„¤μΉ
```bash
# κΈ°λ³Έ λΌμ΄λΈλ¬λ¦¬
pip install torch torchvision torchaudio
pip install transformers datasets accelerate peft
pip install bitsandbytes

# Keras λ° JAX (Gemma μ©)
pip install tensorflow keras keras-nlp jax[cuda]

# Flash Attention (μ„ νƒμ‚¬ν•­ - H100 λ“± μµμ‹  GPU)
pip install flash-attn

# Hugging Face λ° Kaggle
pip install huggingface-hub kaggle
```

### ν™κ²½ μ„¤μ •
```bash
# Hugging Face ν† ν° μ„¤μ •
huggingface-cli login

# Kaggle API μ„¤μ • (Gemma λ¨λΈ λ‹¤μ΄λ΅λ“μ©)
export KAGGLE_USERNAME="your_username"
export KAGGLE_KEY="your_api_key"
```

## π“ μ‚¬μ©λ²•

### 1. Gemma λ¨λΈ νμΈνλ‹
```python
# gemma_fine-tuning.ipynb μ‹¤ν–‰
# - Solidity μ·¨μ•½μ  λ¶„μ„ λ°μ΄ν„°μ…‹ μ‚¬μ©
# - JAX/Keras ν”„λ μ„μ›ν¬ κΈ°λ°
# - CodeGemma 7B λ¨λΈ ν™μ©
```

### 2. LLaMA λ¨λΈ νμΈνλ‹
```python
# llama_fine_tuning.ipynb μ‹¤ν–‰
# - ν•κµ­μ–΄ QA λ°μ΄ν„°μ…‹ μ‚¬μ©
# - PyTorch/Transformers κΈ°λ°
# - QLoRA μµμ ν™” μ μ©
```

### 3. λ¨λΈ μ¶”λ΅  λ° ν…μ¤νΈ
```python
# llm_inference.ipynb μ‹¤ν–‰
# - ν•™μµλ λ¨λΈ λ΅λ“
# - μ„±λ¥ ν‰κ°€ λ° μ¶”λ΅  ν…μ¤νΈ
```

## π― λ°μ΄ν„°μ…‹ μ •λ³΄

### Solidity μ·¨μ•½μ  λ°μ΄ν„°μ…‹
- **νμΌ**: `solidity_vunerability_SWC_datasets_test.jsonl`
- **ν•μ‹**: Instruction-Response μ
- **λ©μ **: μ¤λ§νΈ μ»¨νΈλ™νΈ λ³΄μ• μ·¨μ•½μ  λ¶„λ¥ λ° λ¶„μ„
- **μμ‹**:
  ```json
  {
    "instruction": "Given the code snippet: [Solidity Code]",
    "response": "The vulnerability is...",
    "category": "SWC-101"
  }
  ```

## β™οΈ ν•μ΄νΌνλΌλ―Έν„° μ„¤μ •

### LoRA μ„¤μ •
```python
# μΌλ°μ μΈ LoRA μ„¤μ •
lora_config = LoraConfig(
    r=8,  # rank (r4, r8, r16 μ‹¤ν—)
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1
)
```

### 4λΉ„νΈ μ–‘μν™” μ„¤μ •
```python
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=False
)
```

## π“ λ¨λΈ μ„±λ¥

κ° λ¨λΈλ³„λ΅ λ‹¤μ–‘ν• rank μ„¤μ •(r4, r8, r16)μΌλ΅ μ‹¤ν—μ„ μ§„ν–‰ν•μ—¬ μµμ μ νλΌλ―Έν„°λ¥Ό νƒμƒ‰ν•©λ‹λ‹¤.

### μ €μ¥λ λ¨λΈλ“¤
- `gemma-7b-r8-master/`: Gemma 7B (rank 8)
- `codegemma-7b-r16-master/`: CodeGemma 7B (rank 16)
- κ° λ¨λΈμ— λ€ν•΄ instruction-tuning λ²„μ „λ„ λ³„λ„ μ €μ¥

## π–¥οΈ ν•λ“μ›¨μ–΄ μ”κµ¬μ‚¬ν•­

### κ¶μ¥ ν™κ²½
- **GPU**: NVIDIA H100 (80GB VRAM)
- **λ©”λ¨λ¦¬**: 32GB μ΄μƒ μ‹μ¤ν… RAM
- **μ €μ¥κ³µκ°„**: 100GB μ΄μƒ (λ¨λΈ λ° λ°μ΄ν„°μ…‹ μ €μ¥μ©)

### μµμ ν™” νΉμ§•
- Flash Attention 2 μ§€μ› (H100 ν™κ²½)
- Mixed precision training (bfloat16)
- Gradient checkpointing
- Multi-GPU μ§€μ› (device_map="auto")

## π”¬ μ‹¤ν— κ²°κ³Ό

κ° λ…ΈνΈλ¶μ—μ„ μν–‰ν• μ‹¤ν—λ“¤:
1. **λ¨λΈ ν¬κΈ°λ³„ μ„±λ¥ λΉ„κµ** (2B vs 7B)
2. **LoRA rank μµμ ν™”** (r4, r8, r16)
3. **μ–‘μν™” ν¨κ³Ό λ¶„μ„** (4bit vs full precision)
4. **λ„λ©”μΈλ³„ νΉν™”** (μ½”λ“ vs μΌλ° ν…μ¤νΈ vs ν•κµ­μ–΄)

## π“„ λΌμ΄μ„ μ¤

λ³Έ ν”„λ΅μ νΈλ” μ—°κµ¬ λ©μ μΌλ΅ μ μ‘λμ—μµλ‹λ‹¤. μ‚¬μ©ν•λ” λ¨λΈλ“¤μ λΌμ΄μ„ μ¤λ¥Ό μ¤€μν•΄μ£Όμ„Έμ”:
- Gemma: [Gemma Terms of Use](https://ai.google.dev/gemma/terms)
- LLaMA: [LLaMA 2 License](https://github.com/facebookresearch/llama/blob/main/LICENSE)

## π“ λ¬Έμμ‚¬ν•­

- ν”„λ΅μ νΈ κ΄€λ ¨ λ¬Έμμ‚¬ν•­μ΄ μμΌμ‹λ©΄ Issueλ¥Ό μƒμ„±ν•΄μ£Όμ„Έμ”.
- E-mail: parksu9997@gmail.com


---